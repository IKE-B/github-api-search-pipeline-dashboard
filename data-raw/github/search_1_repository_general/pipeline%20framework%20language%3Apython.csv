name;full_name;description;fork;html_url
transform;tensorflow/transform;Input pipeline framework;false;https://github.com/tensorflow/transform
fuel;mila-iqia/fuel;A data pipeline framework for machine learning;false;https://github.com/mila-iqia/fuel
pipelinewise;transferwise/pipelinewise;Data Pipeline Framework using the singer.io spec;false;https://github.com/transferwise/pipelinewise
mara-pipelines;mara/mara-pipelines;A lightweight opinionated ETL framework, halfway between plain scripts and Apache Airflow;false;https://github.com/mara/mara-pipelines
PipelineDP;OpenMined/PipelineDP;PipelineDP is a Python framework for applying differentially private aggregations to large datasets using batch processing systems such as Apache Spark, Apache Beam, and more.;false;https://github.com/OpenMined/PipelineDP
towhee;towhee-io/towhee;Towhee is a framework that is dedicated to making neural data processing pipelines simple and fast.;false;https://github.com/towhee-io/towhee
datapackage-pipelines;frictionlessdata/datapackage-pipelines;Framework for processing data packages in pipelines of modular components.;false;https://github.com/frictionlessdata/datapackage-pipelines
Niffler;Emory-HITI/Niffler;Niffler: A DICOM Framework for Machine Learning and Processing Pipelines.;false;https://github.com/Emory-HITI/Niffler
vehicle-detection;JunshengFu/vehicle-detection;Created vehicle detection pipeline with two approaches: (1) deep neural networks (YOLO framework) and (2) support vector machines ( OpenCV + HOG).;false;https://github.com/JunshengFu/vehicle-detection
karton;CERT-Polska/karton;Distributed malware processing framework based on Python, Redis and S3.;false;https://github.com/CERT-Polska/karton
pipen;pwwang/pipen;pipen - A pipeline framework for python;false;https://github.com/pwwang/pipen
LightAutoML;sberbank-ai-lab/LightAutoML;LAMA - automatic model creation framework;false;https://github.com/sberbank-ai-lab/LightAutoML
Neuraxle;Neuraxio/Neuraxle;The world's cleanest AutoML library ✨ - Do hyperparameter tuning with the right pipeline abstractions to write clean deep learning production pipelines. Let your pipeline steps have hyperparameter spaces. Design steps in your pipeline like components. Compatible with Scikit-Learn, TensorFlow, and most other libraries, frameworks and MLOps environments.;false;https://github.com/Neuraxio/Neuraxle
bodywork-core;bodywork-ml/bodywork-core;ML pipeline orchestration and model deployments on Kubernetes.;false;https://github.com/bodywork-ml/bodywork-core
jina;jina-ai/jina;☁️ Build multimodal AI applications with cloud-native stack;false;https://github.com/jina-ai/jina
PeekingDuck;aisingapore/PeekingDuck;A modular framework built to simplify Computer Vision inference workloads.;false;https://github.com/aisingapore/PeekingDuck
mosec;mosecorg/mosec;A high-performance ML model serving framework, offers dynamic batching and CPU/GPU pipelines to fully exploit your compute machine;false;https://github.com/mosecorg/mosec
cliboa;BrainPad/cliboa;application framework for ETL(ELT) pipeline, process;false;https://github.com/BrainPad/cliboa
pierogis;pierogis/pierogis;an image and animation processing framework;false;https://github.com/pierogis/pierogis
dlt-meta;databrickslabs/dlt-meta;This is metadata driven DLT based framework for bronze/silver pipelines;false;https://github.com/databrickslabs/dlt-meta
serverless-api-cross-account-cicd;awslabs/serverless-api-cross-account-cicd;Build CI/CD Pipeline for Cross Account Deployment of Lambda API Using Serverless Framework;false;https://github.com/awslabs/serverless-api-cross-account-cicd
pylada-light;pylada/pylada-light;A physics computational framework for python and ipython;false;https://github.com/pylada/pylada-light
dbnd;databand-ai/dbnd;DBND is an agile pipeline framework that helps data engineering teams track and orchestrate their data processes.;false;https://github.com/databand-ai/dbnd
multisensor-pipeline;DFKI-Interactive-Machine-Learning/multisensor-pipeline;The core library of the DFKI multisensor pipeline framework.;false;https://github.com/DFKI-Interactive-Machine-Learning/multisensor-pipeline
bifrost;ledatelescope/bifrost;A stream processing framework for high-throughput applications.;false;https://github.com/ledatelescope/bifrost
Anomaly-Detection-Pipeline-Kedro;kennethleungty/Anomaly-Detection-Pipeline-Kedro;Anomaly Detection Pipeline with Isolation Forest model and Kedro framework;false;https://github.com/kennethleungty/Anomaly-Detection-Pipeline-Kedro
Savior;novioleo/Savior;(WIP)The deployment framework aims to provide a simple, lightweight, fast integrated, pipelined deployment framework for algorithm service that ensures reliability, high concurrency and scalability of services.;false;https://github.com/novioleo/Savior
BioQueue;liyao001/BioQueue;A novel pipeline framework to accelerate bioinformatics analysis;false;https://github.com/liyao001/BioQueue
wishbone;smetj/wishbone;A Python framework to build composable event pipeline servers with minimal effort.;false;https://github.com/smetj/wishbone
llm-app;pathwaycom/llm-app;LLM App is a production framework for building and serving AI applications and LLM-enabled real-time data pipelines.;false;https://github.com/pathwaycom/llm-app
xp;druths/xp;A framework (comand line tool + libraries) for creating flexible compute pipelines;false;https://github.com/druths/xp
corral;toros-astro/corral;The Powerful Pipeline Framework;false;https://github.com/toros-astro/corral
MOT-Tracking-by-Detection-Pipeline;Kazuhito00/MOT-Tracking-by-Detection-Pipeline;Tracking-by-Detection形式のMOT(Multi Object Tracking)について、 DetectionとTrackingの処理を分離して寄せ集めたフレームワーク(Tracking-by-Detection method MOT(Multi Object Tracking) is a framework that separates the processing of Detection and Tracking.);false;https://github.com/Kazuhito00/MOT-Tracking-by-Detection-Pipeline
karton-classifier;CERT-Polska/karton-classifier;File type classifier for the Karton framework.;false;https://github.com/CERT-Polska/karton-classifier
versatile-data-kit;vmware/versatile-data-kit;One framework to develop, deploy and operate data workflows with Python and SQL.;false;https://github.com/vmware/versatile-data-kit
haystack;deepset-ai/haystack;:mag: LLM orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data. With advanced retrieval methods, it's best suited for building RAG, question answering, semantic search or conversational agent chatbots.;false;https://github.com/deepset-ai/haystack
ec2-imagebuilder-poc;Ashex/ec2-imagebuilder-poc;Proof of Concept framework for generating EC2 Image Builder pipelines;false;https://github.com/Ashex/ec2-imagebuilder-poc
snowflake-on-ecs;slalombuild/snowflake-on-ecs;A compact framework for automating a Snowflake analytics pipeline on Amazon ECS.;false;https://github.com/slalombuild/snowflake-on-ecs
cmf;HewlettPackard/cmf;CMF library helps to collect and store information associated with ML pipelines.  It tracks the lineages for artifacts and executions of distributed AI pipelines.  It provides API's to record and query the metadata associated with ML pipelines. The framework adopts a data first approach and all artifacts recorded in the framework are versioned and identified by the content hash.;false;https://github.com/HewlettPackard/cmf
stetl;geopython/stetl;Stetl, Streaming ETL, is a lightweight geospatial processing and ETL framework written in Python.;false;https://github.com/geopython/stetl
HojiChar;HojiChar/HojiChar;The robust text processing pipeline framework enabling customizable, efficient, and metric-logged text preprocessing.;false;https://github.com/HojiChar/HojiChar
predict-census-income;ballet/predict-census-income;" A feature engineering pipeline for income prediction using the Ballet framework ";false;https://github.com/ballet/predict-census-income
Pandora;CNES/Pandora;A stereo matching framework that will help you design your stereo matching pipeline with state of the art performances.;false;https://github.com/CNES/Pandora
Fast-Api-example;KenMwaura1/Fast-Api-example;Simple asynchronous API implemented with Fast-Api framework utilizing Postgres as a Database and SqlAlchemy as ORM . GitHub Actions as CI/CD Pipeline;false;https://github.com/KenMwaura1/Fast-Api-example
pipeless;pipeless-ai/pipeless;An open-source computer vision framework to build and deploy apps in minutes without worrying about multimedia pipelines;false;https://github.com/pipeless-ai/pipeless
ChessAnalysisPipeline;CHESSComputing/ChessAnalysisPipeline;CHESS pipeline framework;false;https://github.com/CHESSComputing/ChessAnalysisPipeline
buildflow;launchflow/buildflow;BuildFlow, is an open source framework for building large scale systems using Python. All you need to do is describe where your input is coming from and where your output should be written, and BuildFlow handles the rest. No configuration outside of the code is required.;false;https://github.com/launchflow/buildflow
verify;lsst/verify;lsst.verify - LSST Science Pipelines Verification Framework;false;https://github.com/lsst/verify
janis-workshops;PMCC-BioinformaticsCore/janis-workshops;Workshops for the Janis pipeline framework;false;https://github.com/PMCC-BioinformaticsCore/janis-workshops
SmartPipeline;giacbrd/SmartPipeline;A framework for rapid development of robust data pipelines following a simple design pattern;false;https://github.com/giacbrd/SmartPipeline
chinstrap;ant4g0nist/chinstrap;A development environment, testing framework, and origination pipeline focused solely on Tezos;false;https://github.com/ant4g0nist/chinstrap
thepipe;tamasgal/thepipe;A simplistic, general purpose pipeline framework.;false;https://github.com/tamasgal/thepipe
PipeLayer;greater-than/PipeLayer;A lightweight, event-driven, pipeline framework. ;false;https://github.com/greater-than/PipeLayer
aws-media-replay-engine;awslabs/aws-media-replay-engine;Media Replay Engine (MRE) is a framework to build automated video clipping and replay (highlight) generation pipelines for live and video-on-demand content.;false;https://github.com/awslabs/aws-media-replay-engine
predict-house-prices;ballet/predict-house-prices;[Playground] A feature engineering pipeline for house price prediction using the Ballet framework;false;https://github.com/ballet/predict-house-prices
karton-config-extractor;CERT-Polska/karton-config-extractor;Static configuration extractor for the Karton framework;false;https://github.com/CERT-Polska/karton-config-extractor
amqpipe;Fatal1ty/amqpipe;Twisted based pipeline framework for AMQP;false;https://github.com/Fatal1ty/amqpipe
ADLStream;pedrolarben/ADLStream;Asynchronous dual-pipeline deep learning framework for online data stream mining;false;https://github.com/pedrolarben/ADLStream
sparklanes;ksbg/sparklanes;A lightweight data processing framework for Apache Spark;false;https://github.com/ksbg/sparklanes
karton-yaramatcher;CERT-Polska/karton-yaramatcher;File and analysis artifacts yara matcher for Karton framework;false;https://github.com/CERT-Polska/karton-yaramatcher
fluidml;fluidml/fluidml;FluidML is a lightweight framework for developing machine learning pipelines.;false;https://github.com/fluidml/fluidml
karton-archive-extractor;CERT-Polska/karton-archive-extractor;Extractor of various archive formats for Karton framework;false;https://github.com/CERT-Polska/karton-archive-extractor
Nifty4Gemini;mrlb05/Nifty4Gemini;A Gemini Data Reduction Pipeline Framework;false;https://github.com/mrlb05/Nifty4Gemini
Rce-KGQA;albert-jin/Rce-KGQA;A novel pipeline framework for multi-hop complex KGQA task. It aims at Improving Multi-hop Embedded Knowledge Graph Question Answering by Introducing Relational Chain Reasoning;false;https://github.com/albert-jin/Rce-KGQA
spark_etl;stonezhong/spark_etl;Generic ETL Pipeline Framework for Apache Spark;false;https://github.com/stonezhong/spark_etl
lpipe;mintel/lpipe;🐑Lambda Pipeline, a framework for writing clear, minimal Python FAAS;false;https://github.com/mintel/lpipe
phylociraptor;reslp/phylociraptor;rapid phylogenomic tree calculator - A highly customizable framework for reproducible phylogenomic inference;false;https://github.com/reslp/phylociraptor
visualblocks;google/visualblocks;Visual Blocks for ML is a Google visual programming framework that lets you create ML pipelines in a no-code graph editor. You – and your users – can quickly prototype workflows by connecting drag-and-drop ML components, including models, user inputs, processors, and visualizations.;false;https://github.com/google/visualblocks
Test-Driven-Development-with-Django-REST-Framework-and-Docker;uwevanopfern/Test-Driven-Development-with-Django-REST-Framework-and-Docker;Test-Driven Development with Django, Django REST Framework, Code Coverage, Code Quality, and Docker;false;https://github.com/uwevanopfern/Test-Driven-Development-with-Django-REST-Framework-and-Docker
builder;unifyai/builder;Build custom Ivy training pipelines for any deep learning framework with clear, hierarchical and robust user specifications;false;https://github.com/unifyai/builder
debussy_concert;debussy-labs/debussy_concert;Debussy is an opinionated Data Architecture and Engineering framework, enabling data analysts and engineers to build better platforms and pipelines.;false;https://github.com/debussy-labs/debussy_concert
pytorch-pipeline;tofunlp/pytorch-pipeline;:dart:Simple ETL Framework for PyTorch;false;https://github.com/tofunlp/pytorch-pipeline
batchout;ilia-khaustov/batchout;Framework for building data pipelines;false;https://github.com/ilia-khaustov/batchout
villard;ariaghora/villard;A pipeline framework for data science projects;false;https://github.com/ariaghora/villard
mlops-workload-orchestrator;aws-solutions/mlops-workload-orchestrator;The MLOps Workload Orchestrator solution helps you streamline and enforce architecture best practices for machine learning (ML) model productionization. This solution is an extendable framework that provides a standard interface for managing ML pipelines for AWS ML services and third-party services.;false;https://github.com/aws-solutions/mlops-workload-orchestrator
aswg-pipeline;PeterEckmann1/aswg-pipeline;Framework and set of tools for analyzing common problems in scientific manuscripts.;false;https://github.com/PeterEckmann1/aswg-pipeline
artellapipe;ArtellaPipe/artellapipe;Framework to create generic Artella based pipelines;false;https://github.com/ArtellaPipe/artellapipe
dtflw;SoleyIo/dtflw;dtflw is a Python framework for building modular data pipelines based on Databricks dbutils.notebook API.;false;https://github.com/SoleyIo/dtflw
pipeline;yifan/pipeline;A data streaming pipeline framework built on Kafka and Pulsar;false;https://github.com/yifan/pipeline
bleachermark;miguelmarco/bleachermark;A pipeline benchmarking framework for python;false;https://github.com/miguelmarco/bleachermark
gnn-classification-pipeline;krzysztoffiok/gnn-classification-pipeline;A framework for experimenting with Graph Neural Networks;false;https://github.com/krzysztoffiok/gnn-classification-pipeline
APACHE_AIRFLOW_DATA_PIPELINES;ultranet1/APACHE_AIRFLOW_DATA_PIPELINES;Project Description: A music streaming company wants to introduce more automation and monitoring to their data warehouse ETL pipelines and they have come to the conclusion that the best tool to achieve this is Apache Airflow. As their Data Engineer, I was tasked to create a reusable production-grade data pipeline that incorporates data quality checks and allows for easy backfills. Several analysts and Data Scientists rely on the output generated by this pipeline and it is expected that the pipeline runs daily on a schedule by pulling new data from the source and store the results to the destination.  Data Description: The source data resides in S3 and needs to be processed in a data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.  Data Pipeline design: At a high-level the pipeline does the following tasks.  Extract data from multiple S3 locations. Load the data into Redshift cluster. Transform the data into a star schema. Perform data validation and data quality checks. Calculate the most played songs for the specified time interval. Load the result back into S3. dag  Structure of the Airflow DAG  Design Goals: Based on the requirements of our data consumers, our pipeline is required to adhere to the following guidelines:  The DAG should not have any dependencies on past runs. On failure, the task is retried for 3 times. Retries happen every 5 minutes. Catchup is turned off. Do not email on retry. Pipeline Implementation:  Apache Airflow is a Python framework for programmatically creating workflows in DAGs, e.g. ETL processes, generating reports, and retraining models on a daily basis. The Airflow UI automatically parses our DAG and creates a natural representation for the movement and transformation of data. A DAG simply is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies. A DAG describes how you want to carry out your workflow, and Operators determine what actually gets done.  By default, airflow comes with some simple built-in operators like PythonOperator, BashOperator, DummyOperator etc., however, airflow lets you extend the features of a BaseOperator and create custom operators. For this project, I developed several custom operators.  operators  The description of each of these operators follows:  StageToRedshiftOperator: Stages data to a specific redshift cluster from a specified S3 location. Operator uses templated fields to handle partitioned S3 locations. LoadFactOperator: Loads data to the given fact table by running the provided sql statement. Supports delete-insert and append style loads. LoadDimensionOperator: Loads data to the given dimension table by running the provided sql statement. Supports delete-insert and append style loads. SubDagOperator: Two or more operators can be grouped into one task using the SubDagOperator. Here, I am grouping the tasks of checking if the given table has rows and then run a series of data quality sql commands. HasRowsOperator: Data quality check to ensure that the specified table has rows. DataQualityOperator: Performs data quality checks by running sql statements to validate the data. SongPopularityOperator: Calculates the top ten most popular songs for a given interval. The interval is dictated by the DAG schedule. UnloadToS3Operator: Stores the analysis result back to the given S3 location. Code for each of these operators is located in the plugins/operators directory.  Pipeline Schedule and Data Partitioning: The events data residing on S3 is partitioned by year (2018) and month (11). Our task is to incrementally load the event json files, and run it through the entire pipeline to calculate song popularity and store the result back into S3. In this manner, we can obtain the top songs per day in an automated fashion using the pipeline. Please note, this is a trivial analyis, but you can imagine other complex queries that follow similar structure.  S3 Input events data:  s3://<bucket>/log_data/2018/11/ 2018-11-01-events.json 2018-11-02-events.json 2018-11-03-events.json .. 2018-11-28-events.json 2018-11-29-events.json 2018-11-30-events.json S3 Output song popularity data:  s3://skuchkula-topsongs/ songpopularity_2018-11-01 songpopularity_2018-11-02 songpopularity_2018-11-03 ... songpopularity_2018-11-28 songpopularity_2018-11-29 songpopularity_2018-11-30 The DAG can be configured by giving it some default_args which specify the start_date, end_date and other design choices which I have mentioned above.  default_args = {     'owner': 'shravan',     'start_date': datetime(2018, 11, 1),     'end_date': datetime(2018, 11, 30),     'depends_on_past': False,     'email_on_retry': False,     'retries': 3,     'retry_delay': timedelta(minutes=5),     'catchup_by_default': False,     'provide_context': True, } How to run this project? Step 1: Create AWS Redshift Cluster using either the console or through the notebook provided in create-redshift-cluster  Run the notebook to create AWS Redshift Cluster. Make a note of:  DWN_ENDPOINT :: dwhcluster.c4m4dhrmsdov.us-west-2.redshift.amazonaws.com DWH_ROLE_ARN :: arn:aws:iam::506140549518:role/dwhRole Step 2: Start Apache Airflow  Run docker-compose up from the directory containing docker-compose.yml. Ensure that you have mapped the volume to point to the location where you have your DAGs.  NOTE: You can find details of how to manage Apache Airflow on mac here: https://gist.github.com/shravan-kuchkula/a3f357ff34cf5e3b862f3132fb599cf3  start_airflow  Step 3: Configure Apache Airflow Hooks  On the left is the S3 connection. The Login and password are the IAM user's access key and secret key that you created. Basically, by using these credentials, we are able to read data from S3.  On the right is the redshift connection. These values can be easily gathered from your Redshift cluster  connections  Step 4: Execute the create-tables-dag  This dag will create the staging, fact and dimension tables. The reason we need to trigger this manually is because, we want to keep this out of main dag. Normally, creation of tables can be handled by just triggering a script. But for the sake of illustration, I created a DAG for this and had Airflow trigger the DAG. You can turn off the DAG once it is completed. After running this DAG, you should see all the tables created in the AWS Redshift.  Step 5: Turn on the load_and_transform_data_in_redshift dag  As the execution start date is 2018-11-1 with a schedule interval @daily and the execution end date is 2018-11-30, Airflow will automatically trigger and schedule the dag runs once per day for 30 times. Shown below are the 30 DAG runs ranging from start_date till end_date, that are trigged by airflow once per day.  schedule;false;https://github.com/ultranet1/APACHE_AIRFLOW_DATA_PIPELINES
synth;BennettDixon/synth;CLI tool for building sets of Docker, Docker Compose, and CI/CD pipeline config files, as well as directory trees and wire-frame files for different frameworks;false;https://github.com/BennettDixon/synth
cognition-pipeline;geospatial-jeff/cognition-pipeline;Python framework for deploying serverless AWS applications.;false;https://github.com/geospatial-jeff/cognition-pipeline
VAI-Lab;AaltoPML/VAI-Lab;Modular pipeline for design assistance. PYPi Package: https://pypi.org/project/vai-lab;false;https://github.com/AaltoPML/VAI-Lab
reip-pipelines;reip-project/reip-pipelines;REIP Software Framework;false;https://github.com/reip-project/reip-pipelines
pypeliner;allonios/pypeliner;a lightweight python framework for creating processing pipelines.;false;https://github.com/allonios/pypeliner
gcp-airflow-foundations;badal-io/gcp-airflow-foundations;Opinionated framework based on Airflow 2.0 for building pipelines to ingest data into a BigQuery data warehouse;false;https://github.com/badal-io/gcp-airflow-foundations
web-scraping-pipeline;ELC/web-scraping-pipeline;This is a demo project to compare two web scrapping frameworks, Playwright and Selenium and using the new Pipelining tool Dagster;false;https://github.com/ELC/web-scraping-pipeline
dataframe-pipeline;IBM/dataframe-pipeline;Machine-learning pipeline framework for pandas and ONNX;false;https://github.com/IBM/dataframe-pipeline
SparkPipelineFramework.Testing;icanbwell/SparkPipelineFramework.Testing;Testing framework that can tests SPF library by just providing input files to setup before running the transformer and output files to use for verifying the output;false;https://github.com/icanbwell/SparkPipelineFramework.Testing
fline;asromahin/fline;Pytorch framework for fast create pipeline;false;https://github.com/asromahin/fline
SIEVE_benchmark_pipeline;szczurek-lab/SIEVE_benchmark_pipeline;Benchmarking framework for SIEVE.;false;https://github.com/szczurek-lab/SIEVE_benchmark_pipeline
luisy;boschglobal/luisy;A Python framework to build reproducible, robust, and scalable data pipelines;false;https://github.com/boschglobal/luisy
reliure;kodexlab/reliure;Minimal framework to manage data processing pipelines;false;https://github.com/kodexlab/reliure
batchy;hsnlab/batchy;Batch-scheduler framework for controlling execution in a packet-processing pipeline based on strict service-level objectives;false;https://github.com/hsnlab/batchy
edgepipes;joakimeriksson/edgepipes;A small python framework for edge computing pipeline processing (Edge ML/AI);false;https://github.com/joakimeriksson/edgepipes
pyplant;gleb-t/pyplant;A framework for out-of-core function pipelines;false;https://github.com/gleb-t/pyplant
mlpipeline;ahmed-shariff/mlpipeline;A framework to consolidate ML workflows;false;https://github.com/ahmed-shariff/mlpipeline
raposa;xurxodiz/raposa;Lexicological framework for pipeline text processing.;false;https://github.com/xurxodiz/raposa
