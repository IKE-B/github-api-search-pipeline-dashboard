search query;name;full_name;description;fork;html_url
pipeline data dashboard visualization;opensearch-catalog;opensearch-project/opensearch-catalog;The OpenSearch Catalog is designed to make it easier for developers and community to contribute, search and install artifacts like plugins, visualization dashboards, ingestion to visualization content packs (data pipeline configurations, normalization, ingestion, dashboards).;false;https://github.com/opensearch-project/opensearch-catalog
pipeline data dashboard visualization;coches-net-dashboard;franloza/coches-net-dashboard;Sample project that use Dagster, dbt, DuckDB and Dash to visualize car and motorcycle Spanish market;false;https://github.com/franloza/coches-net-dashboard
pipeline data dashboard visualization;divvy-bikeshare-de-project;Fozan-Talat/divvy-bikeshare-de-project;An end-to-end data pipeline which extracts divvy bikeshare data from web loads it into data lake and datawarehouse transforms it using dbt and finally , a dashboard to visualize the data using looker studio, the pipeline is orchestrated using prefect;false;https://github.com/Fozan-Talat/divvy-bikeshare-de-project
pipeline data dashboard visualization;ETL-Data-Pipelines-Data-Warehousing-Business-Intelligence-;akashmdubey/ETL-Data-Pipelines-Data-Warehousing-Business-Intelligence-;This project challenged us to process and integrate data coming in from a variety of sources - IMDb data, movie rating data from MovieLens, and The Numbers data. We utilized multiple features of the Talend and SQL server to optimize the complete ETL process    We then visualized the integrated data from our dimensional model in #Tableau and #PowerBI dashboards showcasing some core KPIs and metrics.    This project helped us understand the various challenges faced while building data integration pipelines and business intelligence applications. ;false;https://github.com/akashmdubey/ETL-Data-Pipelines-Data-Warehousing-Business-Intelligence-
pipeline data dashboard visualization;LiveCryptocurrencyDashboard;edoardorealini/LiveCryptocurrencyDashboard;A full Big Data pipeline implemplement in Scala and Python for generating live predictions of crypos prices. Technologies: Python, Scala, Kafka, FB's Prophet, Cassandra and Plotly for final visualization. ;false;https://github.com/edoardorealini/LiveCryptocurrencyDashboard
pipeline data dashboard visualization;Ecommerce-Analytics-Dashboard;Baci-Ak/Ecommerce-Analytics-Dashboard;An E-commerce Analytics Dashboard project showcasing data pipeline automation with Apache NiFi, data analysis with SQL and Python, and interactive data visualization using Tableau.;false;https://github.com/Baci-Ak/Ecommerce-Analytics-Dashboard
pipeline data dashboard visualization;BeerReviewsDataPipeline;directdetour/BeerReviewsDataPipeline;The Beer Reviews Data Pipeline is a data engineering project that involves extracting, preprocessing, and storing beer review data from a Kaggle dataset in a Google Cloud Storage data lake. The data pipeline is built using Python, and Prefect, and includes a Metabase dashboard for data visualization.;false;https://github.com/directdetour/BeerReviewsDataPipeline
pipeline data dashboard visualization;Data-Pipeline-of-NOAA-Storm-Events-and-Sevir-Data;keerti-26/Data-Pipeline-of-NOAA-Storm-Events-and-Sevir-Data;Implementing a data pipeline of SEVIR and Storm Events using Amazon Web Services S3 bucket for storage, Amazon Glue for ETL and Amazon Quicksight for Visualization and Dashboard creation. Implemented a data pipeline using different GCP components, Google Cloud Storage, Datalab, Apache Beam, Dataflow, Google Bigquery and Google Data Studio. Also created a pipeline using Snowflake tables by ingesting the data using Apache Airflow.;false;https://github.com/keerti-26/Data-Pipeline-of-NOAA-Storm-Events-and-Sevir-Data
pipeline data dashboard visualization;IMDB_Database_Warehousing_and_Business_Intelligence;kpkaranpatil600/IMDB_Database_Warehousing_and_Business_Intelligence;Designed Multi Star schema with 10 Fact & 33 Dimension tables & developed Data Integration Pipeline by ETL workflow to load all tables (380M+ rows) from multiple sources such as CSV, MySQL, MSSQL & PostgreSQL by using Talend. Implemented Data Profiling, Error Handling, Load Statistics, Cleansing and Performance Tuning to address data quality gaps & maintained referential integrity using Alteryx & Talend. Executed SQL scripts & created interactive visualization dashboards on PowerBI & Tableau for analyzing the data to draw better insights on sales & customer segmentation;false;https://github.com/kpkaranpatil600/IMDB_Database_Warehousing_and_Business_Intelligence
pipeline data dashboard visualization;Sales-Insight-Dashboard;rushabhgajjar/Sales-Insight-Dashboard;This dashboard is created using Power BI to provide sales insights. Various operations performed include connecting to a data source which is MySQL in this case, data cleaning-ETL in Power Query, manage relationship between tables, create measures using DAX and use them in visualizations, maintain data pipeline during changes/updates to data source, publish report to BI service, create mobile layout to access dashboard from mobile.;false;https://github.com/rushabhgajjar/Sales-Insight-Dashboard
pipeline data dashboard visualization;Data-Warehouse-for-online-store;kingsleydata/Data-Warehouse-for-online-store;This project shows how I create a SQL data warehouse for an online store including how I establish the data pipeline, validate the data, run some analytics on the DATA to find important insights and finally create a visualization dashboard report in Tableau.;false;https://github.com/kingsleydata/Data-Warehouse-for-online-store
pipeline data dashboard visualization;ELT_sensordata;Kibika/ELT_sensordata;The project uses PySpark, MySQL and dbt to create an ELT pipeline. It also uses redash to create a dashboard to visualize the analysis as more data streams in.;false;https://github.com/Kibika/ELT_sensordata
pipeline data dashboard visualization;Data-Analytics-Training-in-Gurgaon-Data-Analytics-Course-in-Gurgaon;sana212/Data-Analytics-Training-in-Gurgaon-Data-Analytics-Course-in-Gurgaon;"Data analytics isn’t just about the future, it is being put to use at this very moment in all businesses. It forms an integral part of the company and the professionals are paid highly for their part. Here are reasons why joining data analytics training in Gurgaon is a viable option   After the completion of Data Analytics Course, you will be able to:   Understand Scala & Apache Spark implementation   Spark operations on Spark Shell   Spark Driver & its related Worker Nodes   Spark + Flume Integration   Setting up Data Pipeline using Apache Flume, Apache Kafka & Spark Streaming   Spark RDDs and Spark Streaming   Spark MLib : Creating Classifiers & Recommendations systems using MLib    Spark Core concepts: Creating of RDDs: Parrallel RDDs, MappedRDD, HadoopRDD, JdbcRDD.   Spark Architecture & Components   Spark SQL experience with CSV, XML & JSON   Reading data from different Spark sources   Spark SQL & Dataframes   Develop and Implement various Machine Learning Algorithms in daily practices & Live Environment   Building Recommendation systems and Classifiers   Perform various type of Analysis (Prediction & Regression)   Implement plotting & graphs using various Machine Learning Libraries   Import data from HDFS & Implement various Machine Learning Models   Building different Neural networks using NumPy and TensorFlow   Power BI Visualization   Power BI Components   Power BI Transformations   Dax functions   Data Exploration and Mapping   Designing Dashboards   Time Series, Aggregation & Filters      Placement   Gyansetu is providing complimentary placement service to all students. Gyansetu Placement Team consistently works on industry collaboration and associations which help our students to find their dream job right after the completion of training.      Why Choose us?   Gyansetu trainers are well known in Industry; who are highly qualified and currently working in top MNCs.   We provide interaction with faculty before the course starts.   Our experts help students in learning Technology from basics, even if you are not good at basic programming skills, don’t worry! We will help you.   Faculties will help you in preparing project reports & presentations.   Students will be provided Mentoring sessions by Experts.    ";false;https://github.com/sana212/Data-Analytics-Training-in-Gurgaon-Data-Analytics-Course-in-Gurgaon
pipeline framework language:python@page2;APF;alercebroker/APF;apf is a framework developed to create a dockerized pipeline to process an alert stream, that can be easily be deployed in a local machine or distributed using Kubernetes.;false;https://github.com/alercebroker/APF
pipeline framework language:python@page2;RoboPyPipeline;we45/RoboPyPipeline;Robot Framework Python Pipeline example;false;https://github.com/we45/RoboPyPipeline
pipeline framework language:python@page2;griptape-flow;griptape-ai/griptape-flow;Python framework for building LLM workflows and pipelines with memory, rules, and chain of thought reasoning.;false;https://github.com/griptape-ai/griptape-flow
pipeline framework language:python@page2;moona;katunilya/moona;Fairly simple railroad-oriented ASGI framework powered by monads and pipelines;false;https://github.com/katunilya/moona
pipeline framework language:python@page2;ParSeq;kklmn/ParSeq;Python software library for Parallel execution of Sequential data analysis.;false;https://github.com/kklmn/ParSeq
pipeline framework language:python@page2;drf-pipeline-views;MrThearMan/drf-pipeline-views;Django REST framework views using the pipeline pattern;false;https://github.com/MrThearMan/drf-pipeline-views
pipeline framework language:python@page2;open-vfx-framework;trinix1975/open-vfx-framework;Visual Effects Pipeline Framework;false;https://github.com/trinix1975/open-vfx-framework
pipeline framework language:python@page2;auto-generated-pipeline-framework;jason-jz-zhu/auto-generated-pipeline-framework;;false;https://github.com/jason-jz-zhu/auto-generated-pipeline-framework
pipeline framework language:python@page2;pipedown;brendanhasz/pipedown;A data science pipelining framework for Python :shushing_face:;false;https://github.com/brendanhasz/pipedown
pipeline framework language:python@page2;robotframework-circlecilibrary;trustedshops-public/robotframework-circlecilibrary;A extension for the Robot Framework to run and manage CircleCI pipelines;false;https://github.com/trustedshops-public/robotframework-circlecilibrary
pipeline framework language:python@page2;midas;bioinformatics-ua/midas;MIDAS is a multi-framework DataLoader that can be leverage to produce highly efficient data pipelines;false;https://github.com/bioinformatics-ua/midas
pipeline framework language:python@page2;niceml;codecentric-oss/niceml;" niceML 🍦 is a Python-based MLOps framework designed to streamline the development and maintenance of machine learning projects, offering efficient and scalable pipelines using TensorFlow and Dagster.";false;https://github.com/codecentric-oss/niceml
pipeline framework language:python@page2;pyngsild;Orange-OpenSource/pyngsild;Python framework that helps you write a pipeline for your Fiware dataflow. It will be an equivalent to pyngsi (NGSIv2) but dedicated to NGSI-LD.;false;https://github.com/Orange-OpenSource/pyngsild
pipeline framework language:python@page2;async-pipelines;alexkeating/async-pipelines;data processing framework;false;https://github.com/alexkeating/async-pipelines
pipeline framework language:python@page2;karton-asciimagic;CERT-Polska/karton-asciimagic;Various decoders for ascii-encoded executables for Karton framework;false;https://github.com/CERT-Polska/karton-asciimagic
pipeline framework language:python@page2;dyda;numbersprotocol/dyda;Dynamic data pipeline framework;false;https://github.com/numbersprotocol/dyda
pipeline framework language:python@page2;Pipeline.py;KOLANICH-libs/Pipeline.py;A framework to build pipelines;false;https://github.com/KOLANICH-libs/Pipeline.py
pipeline framework language:python@page2;sdk-python;go-vela/sdk-python;Python SDK for Vela (Target's official Pipeline Automation Framework);false;https://github.com/go-vela/sdk-python
pipeline framework language:python@page2;Simple-Distributed-Data-Pipeline;YLiao98/Simple-Distributed-Data-Pipeline;This project implements naive query processing operators and provides some data-provenance-tracking functions within the operators. The project also utilizes Ray distributed computing framework to improve scalability.;false;https://github.com/YLiao98/Simple-Distributed-Data-Pipeline
pipeline framework language:python@page2;Knowledge-Distillation-Pipeline;sithu31296/Knowledge-Distillation-Pipeline;PyTorch Knowledge Distillation Framework;false;https://github.com/sithu31296/Knowledge-Distillation-Pipeline
pipeline framework language:python@page2;Luigi-Pipeline;rampk/Luigi-Pipeline;Building a machine learning pipeline using Luigi framework. This project does not focus on modelling accuracy, ability to use a fancy model, or efficiency.  It is mainly about the mechanics of building a proper machine learning pipeline.;false;https://github.com/rampk/Luigi-Pipeline
pipeline framework language:python@page2;karton-autoit-ripper;CERT-Polska/karton-autoit-ripper;AutoIt script ripper for Karton framework;false;https://github.com/CERT-Polska/karton-autoit-ripper
pipeline framework language:python@page2;deba;pckhoi/deba;Environment-agnostic data pipeline framework;false;https://github.com/pckhoi/deba
pipeline framework language:python@page2;data-pipeline-demo-1;AlexandreGarito/data-pipeline-demo-1;This project is a Python-coded and GCP-hosted micro-ETL data pipeline and interactive dashboard that displays API data using the Dash-Plotly web framework, updated daily using DevOps tools such as CI/CD, Docker, and Airflow.;false;https://github.com/AlexandreGarito/data-pipeline-demo-1
pipeline framework language:python@page2;WTU;ag-sc/WTU;Web Table Understanding Framework than parses JSON Web Tables and provides pipeline for annotating/extracting triples;false;https://github.com/ag-sc/WTU
pipeline framework language:python@page2;lambdaedge-serverless-cicd;nikosheng/lambdaedge-serverless-cicd;Based on serverless framework to build up a CICD pipeline for lambda edge development with preexisting CloudFront distribution;false;https://github.com/nikosheng/lambdaedge-serverless-cicd
pipeline framework language:python@page2;MedAI-Framework;LexTran/MedAI-Framework;This repository provides an universal pipeline for medical image segmentation with the support of Pytorch & MONAI.;false;https://github.com/LexTran/MedAI-Framework
pipeline framework language:python@page2;NLP_Pipeline;chaojieji/NLP_Pipeline;Construct generic framework for NLP missions, basing on various third-party libraries.;false;https://github.com/chaojieji/NLP_Pipeline
pipeline framework language:python@page2;snowflet;bluefloyd00/snowflet;Snowflake pipeline, etl, data science framework;false;https://github.com/bluefloyd00/snowflet
pipeline framework language:python@page2;event-pipeline;muxer-dev/event-pipeline;An implementation of an events state machine using AWS Step Functions and the Serverless framework.;false;https://github.com/muxer-dev/event-pipeline
pipeline framework language:python@page2;DataPipelines;DragosManailoiu/DataPipelines;scripts to automate the ETL/ELT process using cloud and orchestration frameworks;false;https://github.com/DragosManailoiu/DataPipelines
pipeline framework language:python@page2;Fast-Api-Vue;KenMwaura1/Fast-Api-Vue;Simple asynchronous API implemented with Fast-Api framework utilizing Postgres as a Database and SqlAlchemy as ORM . GiHub Actions as CI/CD Pipeline. Vue + Daisy UI for the frontend;false;https://github.com/KenMwaura1/Fast-Api-Vue
pipeline framework language:python@page2;gurun;gabrielguarisa/gurun;Task automation framework;false;https://github.com/gabrielguarisa/gurun
pipeline framework language:python@page2;MLDebugger;raonilourenco/MLDebugger;"Code for ""Debugging Machine Learning Pipelines"".  the Algorithms of MLDebugger are now part of BugDoc, a general pipeline debugger framework. This repository will keep the experiments for the DEEM paper,  please check BugDoc's repository for the latest developments.";false;https://github.com/raonilourenco/MLDebugger
pipeline framework language:python@page2;data-science-modeling-framework;msarafzadeh/data-science-modeling-framework;Data Science frame work created to assist the modeling pipeline: from EDA, to model training, to best model selection, to threshold optimization, to model explainabilty.;false;https://github.com/msarafzadeh/data-science-modeling-framework
pipeline framework language:python@page2;FaceDNR-public;systemcorp-ai/FaceDNR-public;Face Detection and Recognition pipeline built on top of OpenCV framework;false;https://github.com/systemcorp-ai/FaceDNR-public
pipeline framework language:python@page2;lightning-streams;BayoAdejare/lightning-streams;Batch/stream ETL pipeline of NOAA GLM dataset, using Python frameworks: Dagster, PySpark and Parquet storage. ;false;https://github.com/BayoAdejare/lightning-streams
pipeline framework language:python@page2;sql-azure-functions-adf-event-trigger;joe-plumb/sql-azure-functions-adf-event-trigger;Code example to demonstrate custom event triggered ADF pipelines driven from metadata frameworks in SQL;false;https://github.com/joe-plumb/sql-azure-functions-adf-event-trigger
pipeline framework language:python@page2;End-to-end-ML-System-Benchmark;hpides/End-to-end-ML-System-Benchmark;A modular suite for benchmarking all stages of Machine Learning pipelines. To find bottlenecks in such pipelines and compare different ML tools, this framework can calculate and visualize several metrics in the data preparation, model training, model validation and inference stages.;false;https://github.com/hpides/End-to-end-ML-System-Benchmark
pipeline framework language:python@page2;ICCP;HaroldBenoit/ICCP;ICCP or Integrated Comfort Controller Pipeline is a framework for research of reinforcement learning based controllers in the context of energy saving and thermal comfort in buildings.;false;https://github.com/HaroldBenoit/ICCP
pipeline framework language:python@page2;pipes;vani-public/pipes;Infrastructure independent task execution framework based on logical task pipelines;false;https://github.com/vani-public/pipes
pipeline framework language:python@page2;ETLite;elau1004/ETLite;A lightweight framework to host your ETL data-pipeline;false;https://github.com/elau1004/ETLite
pipeline framework language:python@page2;wikipedia_data_pipeline;BermanDS/wikipedia_data_pipeline;Data pipeline with using as source - Streaming API from wikipedia.org. The further points in pipeline will be Kafka, Flink framework and PostgreSQL.;false;https://github.com/BermanDS/wikipedia_data_pipeline
pipeline framework language:python@page2;nodework;martindur/nodework;node framework for pipelines, with file handling and processing;false;https://github.com/martindur/nodework
pipeline framework language:python@page2;ToFu;0Miquel/ToFu;TorchFusion (ToFu) is a modular pytorch training pipeline.  It integrates the most well-known frameworks for machine learning like WandB, Hydra and Optuna.;false;https://github.com/0Miquel/ToFu
pipeline framework language:python@page2;arpsas;JordyBottelier/arpsas;This repository contains the framework and experiments that were used for writing the thesis with the title: A Reconfigurable Pipeline for Semi-Automatic Schema Matching;false;https://github.com/JordyBottelier/arpsas
pipeline framework language:python@page2;theflow;trducng/theflow;Pipeline development framework, easy to experiment and compare different pipelines, quick to deploy to workflow orchestration tools;false;https://github.com/trducng/theflow
pipeline framework language:python@page2;spine;shaharluftig/spine;Spine: the backbone of your data pipeline.;false;https://github.com/shaharluftig/spine
pipeline framework language:python@page2;lambda-restapi;gbourniq/lambda-restapi;Quickly develop and deploy serverless REST APIs with FastAPI, AWS Lambda, and AWS API Gateway. This includes a CI/CD pipeline with Travis and the AWS SAM Framework.;false;https://github.com/gbourniq/lambda-restapi
pipeline framework language:python@page2;SCIP;ScalableCytometryImageProcessing/SCIP;Scalable Cytometry Image Processing (SCIP) is an open-source tool that implements an image processing pipeline on top of Dask, a distributed computing framework written in Python. SCIP performs projection, illumination correction, image segmentation and masking, and feature extraction.;false;https://github.com/ScalableCytometryImageProcessing/SCIP
pipeline framework language:python@page2;phd_template;sbucaille/phd_template;Template to create training pipeline using PytorchLighning, Hydra and DVC frameworks;false;https://github.com/sbucaille/phd_template
pipeline framework language:python@page2;betl;brianspurling/betl;A Kimball ETL (Extract, Transform, Load) framework for building robust data pipelines in Python;false;https://github.com/brianspurling/betl
pipeline framework language:python@page2;ipipeline;novaenext/ipipeline;A micro framework to build and execute pipelines from different domains.;false;https://github.com/novaenext/ipipeline
pipeline framework language:python@page2;astromlp;nunorc/astromlp;A framework for building deep learning models and pipelines for astrophysics applications.;false;https://github.com/nunorc/astromlp
pipeline framework language:python@page2;fastapi-localtrack;mbari-org/fastapi-localtrack;Lightweight Machine Learning API for running video tracking pipelines powered by the FastAPI framework;false;https://github.com/mbari-org/fastapi-localtrack
pipeline framework language:python@page2;treasure-data-digdag-graph;anilkulkarni87/treasure-data-digdag-graph;How to visualize digdag? digdag is an open source etl pipeline framework by Treasure-Data;false;https://github.com/anilkulkarni87/treasure-data-digdag-graph
pipeline framework language:python@page2;lavactl;alfonsoros/lavactl;tool ment to help with the setup of CI pipelines using Linaro's LAVA testing framework;false;https://github.com/alfonsoros/lavactl
pipeline framework language:python@page2;Deploying_Scalable_ML_Pipeline_in_Production;Minhvt34/Deploying_Scalable_ML_Pipeline_in_Production;This repo to develop a classification model by applying unit tests to monitor the model performance on various slices of the data. Then, deploying the model using FastAPI package and create API tests. Both the slice-validation and the API tests will be incorporated into a CI/CD framework using GitHub Actions. ;false;https://github.com/Minhvt34/Deploying_Scalable_ML_Pipeline_in_Production
pipeline framework language:python@page2;CCAugmentation;pijuszczyk/CCAugmentation;Data preprocessing & augmentation framework, designed for working with crowd counting datasets, ML/DL framework-independent. Supports multitude of simple as well as advanced transformations, outputs and loaders, all of them to be combined using pipelines.;false;https://github.com/pijuszczyk/CCAugmentation
pipeline framework language:python@page2;spydy;superjcd/spydy;基于Pipeline的爬虫框架， 工作流非常简单、直观， 而且支持异步。light-weight high-level web-crawling framework;false;https://github.com/superjcd/spydy
pipeline framework language:python@page2;vehicle-detection-system-in-python;Aklilu-Mandefro/vehicle-detection-system-in-python;Created vehicle detection pipeline with two approaches: (1) deep neural networks (YOLO framework) and (2) support vector machines ( OpenCV + HOG). ;false;https://github.com/Aklilu-Mandefro/vehicle-detection-system-in-python
pipeline framework language:python@page2;SigmaParticle;dzyphr/SigmaParticle;framework for interacting with ergo blockchain based on ergpy to be used in cross chain pipeline extensions;false;https://github.com/dzyphr/SigmaParticle
pipeline framework language:python@page2;Point-of-sale-Python-Django-CICD;santiagortiiz/Point-of-sale-Python-Django-CICD;Point of sales application developed in Django and REST framework with CI/CD pipelines with GitHub actions;false;https://github.com/santiagortiiz/Point-of-sale-Python-Django-CICD
pipeline framework language:python@page2;laplace-gnn-recommendation;dream-faster/laplace-gnn-recommendation;⛓️ laplace is an end-to-end ML framework to train and predict on neurally-enhanced graphs for recommendation.  The pipeline is designed for self-supervised edge prediction on heterogenous graphs.;false;https://github.com/dream-faster/laplace-gnn-recommendation
pipeline framework language:python@page2;pachelm;sinonkt/pachelm;Pachelm [Pla - chelm] (Pachyderm + helm) helm style + laravel migration & seeds style framework to manage pipeline deployment as a package of migrations.;false;https://github.com/sinonkt/pachelm
pipeline framework language:python@page2;lambda-functions-actions;rribeiro1/lambda-functions-actions;AWS Lambda function implemented with Serverless framework and CI pipeline using GitHub actions. It also integrates with CodeClimate for test coverage reports.;false;https://github.com/rribeiro1/lambda-functions-actions
pipeline framework language:python@page2;happy-tasks;garrylachman/happy-tasks;"""Happy Tasks"" is an Python Framework for building tasks based batch processes. A network of pipelines, triggers, dependencies, workflow management, reporting & scheduling.";false;https://github.com/garrylachman/happy-tasks
pipeline framework language:python@page2;FastApi-Signoz-App;KenMwaura1/FastApi-Signoz-App;Simple asynchronous API implemented with Fast-Api framework utilizing Postgres as a Database and SqlAlchemy as ORM . GitHub Actions as CI/CD Pipeline;false;https://github.com/KenMwaura1/FastApi-Signoz-App
pipeline framework language:python@page2;mastering_mediapipe;bhav09/mastering_mediapipe;MediaPipe is a an open-source framework from Google for building multimodal (eg. video, audio, any time series data), cross platform (i.e Android, iOS, web, edge devices) applied ML pipelines. It is performance optimized with end-to-end on device inference in mind.;false;https://github.com/bhav09/mastering_mediapipe
pipeline framework language:python@page2;machine-learning-hyperparameter-optimization-app;ishani-chakraborty/machine-learning-hyperparameter-optimization-app;Machine Learning App using the StreamLit web framework that aims to eliminate the barrier in understanding machine learning model building by streamlining the process thereby allowing non-technical users to harness the power of machine learning through data visualization and input customization;false;https://github.com/ishani-chakraborty/machine-learning-hyperparameter-optimization-app
pipeline framework language:python@page2;api_project;koomani/api_project;Building a full API in Python using FastAPI along with HTTP requests tested by postman tool, Sql database connections, automation testing with pytest framework, and building  CI/CD pipelines;false;https://github.com/koomani/api_project
pipeline framework language:python@page2;Clockwork;facebookresearch/Clockwork;Recurring batch data pipelines are a staple of the modern enterprisescale data warehouse. As a data warehouse scales to support more products and services, a growing number of interdependent pipelines running at various cadences can give rise to periodic resource bottlenecks for a cluster. This resource contention results in pipelines starting at unpredictable times each day and consequently variable landing times for the data artifacts they produce. The variability gets compounded by the dependency structure of the workload, and the resulting unpredictability can disrupt the project workstreams which consume this data. We present Clockwork, a delay-based global scheduling framework for data pipelines which improves landing time stability by spreading out tasks throughout the day. Whereas most scheduling algorithms optimize for makespan or average job completion times, Clockwork’s execution plan optimizes for stability in task completion times while also targeting predifined pipeline. Online experiments comparing this novel scheduling algorithm and a previously proposed greedy procrastinating heurstic show tasks complete almost an hour earlier on average, while exhibiting lower landing time variance and producing significantly less competition for resources in a target cluster.;false;https://github.com/facebookresearch/Clockwork
pipeline framework language:python@page2;python-devops;Viha27/python-devops;A Devops pipeline is set of automated processes and tools that the development (Dev) and operations (Ops) teams implement to build, test, and deploy software faster and easier.  In this course you will complete DevOps pipeline generally consists of a set of tools which are normally broken down into the following categories:  Plan  Code  Integrate  Test  Release  Deploy  Operate  This learning path will cover:  Git is an open-source and distributed version control system.  Github is git repository hosting service used for code sharing, bug tracking, feature request and much more.    PyCharm is an integrated development environment (IDE) for python programing language.  Flask is a python web framework.  HTML is the standard markup language for Web pages.  CSS is a style sheet language use to style a HTML document.  SQLAlchemy is an open-source SQL toolkit and object-relational mapper which gives full power and flexibility of SQL.    Selenium is used to automate web browser interaction.  Pytest is unit testing framework that allows users to write test codes.    Ngrok allows to expose a web server running on your local machine to the internet.    Github Action enables you to include Continues Integration (CI) and continuous deployment (CD) capabilities and many other features directly in your repository.    Docker is an open source containerization platform enables developers to package applications into containers.  Docker Hub is a cloud-based repository for finding and sharing container images with your team.  Kubernetes is an open-source container orchestration for automating deployment, scaling, and management of containerized applications.  This course is one stop shop where you will learn web development, continuous integration, continuous deployment, containerization, writing neat and quality code, devops concepts and much more with python programing language.  What you’ll learn Learn to build Continuous Integration Continuous Deployment pipeline Build CI CD tool to update docker image after any update Learn to create dockerfile Learn the fundamental concepts of Docker Learn the fundamental concepts of Kubernetes Learn to create Kubernetes YAML files Learn to deploy high availability, fault tolerance, scalable application Learn all the basic and advanced git commands Learn different types of branches like master, developer, feature, release and hotfix branch Learn fundamental concepts of Version Control System Learn to use Github actions for CI CD pipeline Learn to build python flask web application Learn to use SQL Alchemy Lean to create HTML pages using HTML, CSS and bootstrap Are there any course requirements or prerequisites? Git installed Docker installed Kubernetes installed Any IDE Github account Docker hub account Who this course is for: Anyone who wants to Enhance their skills in DevOps domain Developers and IT Pros Instructor User photo Pranjal Srivastava Docker | Kubernetes | AWS | Azure | ML | Linux | Python  I am an Instructor, Devops engineer, machine learning enthusiast, cloud expert and passionate developer.  I have authored 60+ courses with over 50,000+ students worldwide across 175+ countries on wide array of technologies like containerization, machine learning, Linux, programming languages and cloud computing platforms like Microsoft Azure, Amazon Web Service and IBM Cloud.;false;https://github.com/Viha27/python-devops
pipeline framework language:python@page2;AI-Attention-assist-system-using-MediaPipe;Arwindhraj/AI-Attention-assist-system-using-MediaPipe;Utilizing MediaPipe, a cross-platform framework for creating computer vision pipelines, we created an attention support system. The system uses EAR and MAR to assess the driver's level of drowsiness. An alarm will sound to warn the driver if they are becoming sleepy.;false;https://github.com/Arwindhraj/AI-Attention-assist-system-using-MediaPipe
data warehouse analysis language:python;CueObserve;cuebook/CueObserve;Timeseries Anomaly detection and Root Cause Analysis on data in SQL data warehouses and databases;false;https://github.com/cuebook/CueObserve
data warehouse analysis language:python;udend-data-warehouse-project;jonathankamau/udend-data-warehouse-project;Sparkify Data Warehouse Project for song play analysis;false;https://github.com/jonathankamau/udend-data-warehouse-project
data warehouse analysis language:python;ixmp4;iiasa/ixmp4;A data warehouse for high-powered scenario analysis in the domain of integrated assessment of climate change and energy systems modeling;false;https://github.com/iiasa/ixmp4
data warehouse analysis language:python;runpandarun;simonwoerpel/runpandarun;A simple interface for reproducible & persistent data warehousing for small data analysis projects;false;https://github.com/simonwoerpel/runpandarun
data warehouse analysis language:python;DATA-WAREHOUSE-PROJECT-FOR-MUSIC-DATA-ANALYSIS;Sarensanth/DATA-WAREHOUSE-PROJECT-FOR-MUSIC-DATA-ANALYSIS;;false;https://github.com/Sarensanth/DATA-WAREHOUSE-PROJECT-FOR-MUSIC-DATA-ANALYSIS
data warehouse analysis language:python;mearching-learning-project;Broad-sky/mearching-learning-project;⭐This warehouse mainly describes data understanding, data visualization, data preprocessing, feature selection, model selection, model evaluation and the use of various machine learning algorithms based on sklearn library (univariate feature selection, recursive feature elimination, principal component analysis, decision tree , random forest, GBDT family (boosting algorithm)).;false;https://github.com/Broad-sky/mearching-learning-project
data warehouse analysis language:python;data-warehouse-project-redshift;hudson-pierce/data-warehouse-project-redshift;Demo code for a Redshift Data Warehouse and ETL job which prepares the data for analysis.;false;https://github.com/hudson-pierce/data-warehouse-project-redshift
data warehouse analysis language:python;Analysis-of-Hospital-Pricing-Data;ddziebol/Analysis-of-Hospital-Pricing-Data;SQL and data warehousing project compiling and analyzing newly publicly available hospital pricing data.;false;https://github.com/ddziebol/Analysis-of-Hospital-Pricing-Data
data warehouse analysis language:python;Sparkify-Data-warehouse;sagarpednekar/Sparkify-Data-warehouse;The project involves developing an ETL pipeline on Amazon Redshift to load data from S3, create staging tables, and transform the data into a star schema optimized for song play analysis. The goal is to enable Sparkify's analytical team to uncover insights about user behavior and music preferences.;false;https://github.com/sagarpednekar/Sparkify-Data-warehouse
data warehouse analysis language:python;Data-Science;Viapa/Data-Science;The warehouse is mainly used for data analysis and preprocessing in machine learning;false;https://github.com/Viapa/Data-Science
data warehouse analysis language:python;TripAdvisor-Reviews-Analysis-AZURE;NishanthRajkumar/TripAdvisor-Reviews-Analysis-AZURE;Trip Advisor Reviews Sentiment Analysis Project using Azure synapse analytics, Self Hosted IR, Data Lake, Data Warehouse, Power BI and Azure ML;false;https://github.com/NishanthRajkumar/TripAdvisor-Reviews-Analysis-AZURE
data warehouse analysis language:python;APACHE_AIRFLOW_DATA_PIPELINES;ultranet1/APACHE_AIRFLOW_DATA_PIPELINES;Project Description: A music streaming company wants to introduce more automation and monitoring to their data warehouse ETL pipelines and they have come to the conclusion that the best tool to achieve this is Apache Airflow. As their Data Engineer, I was tasked to create a reusable production-grade data pipeline that incorporates data quality checks and allows for easy backfills. Several analysts and Data Scientists rely on the output generated by this pipeline and it is expected that the pipeline runs daily on a schedule by pulling new data from the source and store the results to the destination.  Data Description: The source data resides in S3 and needs to be processed in a data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.  Data Pipeline design: At a high-level the pipeline does the following tasks.  Extract data from multiple S3 locations. Load the data into Redshift cluster. Transform the data into a star schema. Perform data validation and data quality checks. Calculate the most played songs for the specified time interval. Load the result back into S3. dag  Structure of the Airflow DAG  Design Goals: Based on the requirements of our data consumers, our pipeline is required to adhere to the following guidelines:  The DAG should not have any dependencies on past runs. On failure, the task is retried for 3 times. Retries happen every 5 minutes. Catchup is turned off. Do not email on retry. Pipeline Implementation:  Apache Airflow is a Python framework for programmatically creating workflows in DAGs, e.g. ETL processes, generating reports, and retraining models on a daily basis. The Airflow UI automatically parses our DAG and creates a natural representation for the movement and transformation of data. A DAG simply is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies. A DAG describes how you want to carry out your workflow, and Operators determine what actually gets done.  By default, airflow comes with some simple built-in operators like PythonOperator, BashOperator, DummyOperator etc., however, airflow lets you extend the features of a BaseOperator and create custom operators. For this project, I developed several custom operators.  operators  The description of each of these operators follows:  StageToRedshiftOperator: Stages data to a specific redshift cluster from a specified S3 location. Operator uses templated fields to handle partitioned S3 locations. LoadFactOperator: Loads data to the given fact table by running the provided sql statement. Supports delete-insert and append style loads. LoadDimensionOperator: Loads data to the given dimension table by running the provided sql statement. Supports delete-insert and append style loads. SubDagOperator: Two or more operators can be grouped into one task using the SubDagOperator. Here, I am grouping the tasks of checking if the given table has rows and then run a series of data quality sql commands. HasRowsOperator: Data quality check to ensure that the specified table has rows. DataQualityOperator: Performs data quality checks by running sql statements to validate the data. SongPopularityOperator: Calculates the top ten most popular songs for a given interval. The interval is dictated by the DAG schedule. UnloadToS3Operator: Stores the analysis result back to the given S3 location. Code for each of these operators is located in the plugins/operators directory.  Pipeline Schedule and Data Partitioning: The events data residing on S3 is partitioned by year (2018) and month (11). Our task is to incrementally load the event json files, and run it through the entire pipeline to calculate song popularity and store the result back into S3. In this manner, we can obtain the top songs per day in an automated fashion using the pipeline. Please note, this is a trivial analyis, but you can imagine other complex queries that follow similar structure.  S3 Input events data:  s3://<bucket>/log_data/2018/11/ 2018-11-01-events.json 2018-11-02-events.json 2018-11-03-events.json .. 2018-11-28-events.json 2018-11-29-events.json 2018-11-30-events.json S3 Output song popularity data:  s3://skuchkula-topsongs/ songpopularity_2018-11-01 songpopularity_2018-11-02 songpopularity_2018-11-03 ... songpopularity_2018-11-28 songpopularity_2018-11-29 songpopularity_2018-11-30 The DAG can be configured by giving it some default_args which specify the start_date, end_date and other design choices which I have mentioned above.  default_args = {     'owner': 'shravan',     'start_date': datetime(2018, 11, 1),     'end_date': datetime(2018, 11, 30),     'depends_on_past': False,     'email_on_retry': False,     'retries': 3,     'retry_delay': timedelta(minutes=5),     'catchup_by_default': False,     'provide_context': True, } How to run this project? Step 1: Create AWS Redshift Cluster using either the console or through the notebook provided in create-redshift-cluster  Run the notebook to create AWS Redshift Cluster. Make a note of:  DWN_ENDPOINT :: dwhcluster.c4m4dhrmsdov.us-west-2.redshift.amazonaws.com DWH_ROLE_ARN :: arn:aws:iam::506140549518:role/dwhRole Step 2: Start Apache Airflow  Run docker-compose up from the directory containing docker-compose.yml. Ensure that you have mapped the volume to point to the location where you have your DAGs.  NOTE: You can find details of how to manage Apache Airflow on mac here: https://gist.github.com/shravan-kuchkula/a3f357ff34cf5e3b862f3132fb599cf3  start_airflow  Step 3: Configure Apache Airflow Hooks  On the left is the S3 connection. The Login and password are the IAM user's access key and secret key that you created. Basically, by using these credentials, we are able to read data from S3.  On the right is the redshift connection. These values can be easily gathered from your Redshift cluster  connections  Step 4: Execute the create-tables-dag  This dag will create the staging, fact and dimension tables. The reason we need to trigger this manually is because, we want to keep this out of main dag. Normally, creation of tables can be handled by just triggering a script. But for the sake of illustration, I created a DAG for this and had Airflow trigger the DAG. You can turn off the DAG once it is completed. After running this DAG, you should see all the tables created in the AWS Redshift.  Step 5: Turn on the load_and_transform_data_in_redshift dag  As the execution start date is 2018-11-1 with a schedule interval @daily and the execution end date is 2018-11-30, Airflow will automatically trigger and schedule the dag runs once per day for 30 times. Shown below are the 30 DAG runs ranging from start_date till end_date, that are trigged by airflow once per day.  schedule;false;https://github.com/ultranet1/APACHE_AIRFLOW_DATA_PIPELINES
data warehouse analysis language:python;GCLAP;bzambri/GCLAP;[Insight Data Engineering project] A climate data warehouse that enables the immediate analysis of large climate datasets without data preparation.;false;https://github.com/bzambri/GCLAP
data warehouse analysis language:python;py-entr;entralliance/py-entr;The ENTR Alliance's python package to load data from an ENTR Warehouse to perform analysis on these data using OpenOA.;false;https://github.com/entralliance/py-entr
data warehouse analysis language:python;ETL_For_HL7;meysam24zamani/ETL_For_HL7;Here you have access to the full code of ETL_for_HL7. This ETL is created in order to extract patient information from various data sources, transform them into the analysis-ready format, and finally load it into the data warehouse.;false;https://github.com/meysam24zamani/ETL_For_HL7
pipeline data science language:python;galaxy;galaxyproject/galaxy;Data intensive science for everyone.;false;https://github.com/galaxyproject/galaxy
pipeline data science language:python;datajoint-python;datajoint/datajoint-python;Relational data pipelines for the science lab ;false;https://github.com/datajoint/datajoint-python
pipeline data science language:python;kedro;kedro-org/kedro;Kedro is a toolbox for production-ready data science. It uses software engineering best practices to help you create data engineering and data science pipelines that are reproducible, maintainable, and modular.;false;https://github.com/kedro-org/kedro
pipeline data science language:python;aws-step-functions-data-science-sdk-python;aws/aws-step-functions-data-science-sdk-python;Step Functions Data Science SDK for building machine learning (ML) workflows and pipelines on AWS;false;https://github.com/aws/aws-step-functions-data-science-sdk-python
pipeline data science language:python;deepvariant;google/deepvariant;DeepVariant is an analysis pipeline that uses a deep neural network to call genetic variants from next-generation DNA sequencing data.;false;https://github.com/google/deepvariant
pipeline data science language:python;accelerated-data-science;oracle/accelerated-data-science;ADS is the Oracle Data Science Cloud Service's python SDK supporting, model ops (train/eval/deploy), along with running workloads on Jobs and Pipeline resources.;false;https://github.com/oracle/accelerated-data-science
pipeline data science language:python;seq2science;vanheeringen-lab/seq2science;Automated and customizable preprocessing of Next-Generation Sequencing data, including full (sc)ATAC-seq, ChIP-seq, and (sc)RNA-seq workflows. Works equally easy with public as local data. ;false;https://github.com/vanheeringen-lab/seq2science
pipeline data science language:python;disdat;kyocum/disdat;Data science tool for creating and deploying pipelines with versioned data ;false;https://github.com/kyocum/disdat
pipeline data science language:python;babyds;Rock-River-Research/babyds;An AI-powered data science pipeline;false;https://github.com/Rock-River-Research/babyds
pipeline data science language:python;NIRSPEC-Data-Reduction-Pipeline;Keck-DataReductionPipelines/NIRSPEC-Data-Reduction-Pipeline;NSDRP was developed by the Keck Observatory Archive.  KOA is a collaboration between the NASA Exoplanet Science Institute and the W. M. Keck Observatory. NExScI is sponsored by NASA’s Exoplanet Program and operated by the California Institute of Technology in coordination with the Jet Propulsion Laboratory. Please contact the KOA Help Desk with questions: https://koa.ipac.caltech.edu/cgi-bin/Helpdesk/nph-genTicketForm?projname=KOA;false;https://github.com/Keck-DataReductionPipelines/NIRSPEC-Data-Reduction-Pipeline
pipeline data science language:python;DevOpsForAI;praneet22/DevOpsForAI;Deploy end to end ML/AI pipelines using Azure ML Service and Azure DevOps and take a Data Science (AI/ML) solution into Production.;false;https://github.com/praneet22/DevOpsForAI
pipeline data science language:python;ci-cd-pipeline-template-for-data-projects;zoltan-nz/ci-cd-pipeline-template-for-data-projects;CI/CD pipeline template for data science projects using GitLab CI and Kubernetes;false;https://github.com/zoltan-nz/ci-cd-pipeline-template-for-data-projects
pipeline data science language:python;prodmodel;prodmodel/prodmodel;Build, test, deploy, iterate - Dev and prod tool for data science pipelines;false;https://github.com/prodmodel/prodmodel
pipeline data science language:python;Deployment_Data_Science_Project;sawadogosalif/Deployment_Data_Science_Project;Deploy maching learning model in local / server /API /cloud/ dockers;false;https://github.com/sawadogosalif/Deployment_Data_Science_Project
pipeline data science language:python;DS-Pipeline;sumonbis/DS-Pipeline;This repository contains the source code and data used for the ICSE'22 paper on Data Science Pipeline.;false;https://github.com/sumonbis/DS-Pipeline
pipeline data science language:python;aqueduct;CityOfLosAngeles/aqueduct;A shared pipeline for building ETLs and batch jobs that we run at the City of LA for Data Science Projects. Built on Apache Airflow & Civis Platform ;false;https://github.com/CityOfLosAngeles/aqueduct
pipeline data science language:python;dsds;abstractqqq/dsds;Another take on data science pipelines.;false;https://github.com/abstractqqq/dsds
pipeline data science language:python;mlpl;ozanzgur/mlpl;A machine learning pipeline to speed up data science life cycle;false;https://github.com/ozanzgur/mlpl
pipeline data science language:python;villard;ariaghora/villard;A pipeline framework for data science projects;false;https://github.com/ariaghora/villard
pipeline data science language:python;python-snippets;dushyantkhosla/python-snippets;Useful Python snippets for data science and more;false;https://github.com/dushyantkhosla/python-snippets
pipeline data science language:python;CMIDAT01K-DATA-SCIENCE-for-IOT;robvdw/CMIDAT01K-DATA-SCIENCE-for-IOT;This course is designed to provide a basic grounding of the Internet of Things (IoT) and Data Science with a practical knowledge of the Raspberry Pi as the main device to build a consumer  IoT data pipeline. The aim is to equip students with hands-on experience to solve  basic IoT problems. Providing them with templates and a data-toolkit (code), which can be implemented through popular single board microcontrollers (Arduino and/or Raspberry Pi).;false;https://github.com/robvdw/CMIDAT01K-DATA-SCIENCE-for-IOT
pipeline data science language:python;dia_pipe;LSSTDESC/dia_pipe;Difference Image Analysis pipeline using LSST DM Science Pipelines developed for DESC Data Challenges;false;https://github.com/LSSTDESC/dia_pipe
pipeline data science language:python;unit-tests-ml-pipelines;agaravaglia/unit-tests-ml-pipelines;Repository example of implementing unit-tests for data science projects;false;https://github.com/agaravaglia/unit-tests-ml-pipelines
pipeline data science language:python;Piscine_Python_Data_Science.42;Chegashi/Piscine_Python_Data_Science.42;The piscine focuses on basic programming skills with Python and the most popular and useful data science libraries. The participants will be able to collect data with parsing, preprocess data using Pandas and SQL and build pipelines with machine learning algorithms.;false;https://github.com/Chegashi/Piscine_Python_Data_Science.42
pipeline data science language:python;datanectar;wesmadrigal/datanectar;A project for streamlining large scale ETL and data science pipelines.;false;https://github.com/wesmadrigal/datanectar
pipeline data science language:python;tv-show-recommendations;itsjafer/tv-show-recommendations;Machine learning pipeline trained offline that, given a TV Show, recommends 10 similar TV Shows using cosine similarities based on a variety of features;false;https://github.com/itsjafer/tv-show-recommendations
pipeline data science language:python;stau;connectedcars/stau;Data science pipeline tool for handling recurring workloads;false;https://github.com/connectedcars/stau
pipeline data science language:python;FRB_pipeline;federatedcloud/FRB_pipeline;A customizable scientific software pipeline for detecting, categorizing, and viewing single pulse candidates that may be Fast Radio Burst (FRB) sources in Radio Astronomy data.;false;https://github.com/federatedcloud/FRB_pipeline
pipeline data science language:python;linkedin-data-science-job-tracker;napalm5/linkedin-data-science-job-tracker;Data pipeline with web interface to track real-time trends in the Data Science job market;false;https://github.com/napalm5/linkedin-data-science-job-tracker
pipeline data science language:python;DataSciencePipline;Serena-TT/DataSciencePipline;Data_science_pipeline;false;https://github.com/Serena-TT/DataSciencePipline
pipeline data science language:python;data_science_pipeline;nickruta/data_science_pipeline;This project demonstrates all of the technologies needed to create an end-to-end data science pipeline. This includes consuming data from an original source, processing and storing it and finally providing machine-learning based results to end users.;false;https://github.com/nickruta/data_science_pipeline
pipeline data science language:python;ploomber-pipeline-demo;noratenk/ploomber-pipeline-demo;A demo project using ploomber to demonstate data science pipeline using Jupyter notebooks as building blocks.;false;https://github.com/noratenk/ploomber-pipeline-demo
pipeline data science language:python;dspipeline;chrisferreyra13/dspipeline;Ultimate data science pipeline;false;https://github.com/chrisferreyra13/dspipeline
pipeline data science language:python;Science_report;TuanLe-BenKon/Science_report;Repo of online data pipeline and dashboard;false;https://github.com/TuanLe-BenKon/Science_report
pipeline data science language:python;DSND_Data_Pipeline;hendrik-sill/DSND_Data_Pipeline;Data Science Nanodegree ETL and Machine Learning Pipeline Project;false;https://github.com/hendrik-sill/DSND_Data_Pipeline
pipeline data science language:python;udacity_disaster_response_pipeline_project;feliche93/udacity_disaster_response_pipeline_project;Project for the Udacity Data Science Nanodegree;false;https://github.com/feliche93/udacity_disaster_response_pipeline_project
pipeline data science language:python;data_provenance_for_data_science;Lucass97/data_provenance_for_data_science;Backend with the purpose of capturing data provenance and metadata of a preprocessing Pipeline;false;https://github.com/Lucass97/data_provenance_for_data_science
pipeline data science language:python;AutomaticDataScience;bilan604/AutomaticDataScience;Automating the entire data science pipeline from loading the data to comparing trained model performance metrics;false;https://github.com/bilan604/AutomaticDataScience
pipeline data science language:python;DisasterResponsePipeline;carsimoes/DisasterResponsePipeline;A Data Science project. #Python #JupyterNotebook #MachineLearning #DataMining;false;https://github.com/carsimoes/DisasterResponsePipeline
pipeline data science language:python;Disaster-Response-pipeline-Project;AyaAbulnasr/Disaster-Response-pipeline-Project;Using Data Science with Machine Learning techniques (ETL pipeline and ML pipeline) to classify received messages after disasters.;false;https://github.com/AyaAbulnasr/Disaster-Response-pipeline-Project
pipeline data science language:python;ML-airport-taxi-out;nasa/ML-airport-taxi-out;The ML-airport-taxi-out software is developed to provide a reference implementation to serve as a research example how to train and register Machine Learning (ML) models intended for four distinct use cases: 1) unimpeded AMA taxi out, 2) unimpeded ramp taxi out, 3) impeded AMA taxi out, and 4) impeded ramp taxi out. The software is designed to point to databases which are not provided as part of the software release and thus this software is only intended to serve as an example of best practices. The software is built in python and leverages open-source libraries kedro, scikitlearn, MLFlow, and others. The software provides examples how to build three distinct pipelines for data query and save, data engineering, and data science. These pipelines enable scalable, repeatable, and maintainable development of ML models.;false;https://github.com/nasa/ML-airport-taxi-out
pipeline data science language:python;ML-airport-data-services;nasa/ML-airport-data-services;The ML-airport-data-services software is developed to provide common code used throughout the ML-airport suite of software. The software is built in python and leverages open-source libraries kedro, scikitlearn, MLFlow, and others. The software provides useful functions for development of pipelines including data query and save, data engineering, and data science.;false;https://github.com/nasa/ML-airport-data-services
pipeline data science language:python;ML-airport-configuration;nasa/ML-airport-configuration;The ML-airport-configuration software is developed to provide a reference implementation to serve as a research example how to train and register Machine Learning (ML) models intended for predicting airport configuration as a time series. The software is designed to point to databases which are not provided as part of the software release and thus this software is only intended to serve as an example of best practices. The software is built in python and leverages open-source libraries kedro, scikitlearn, MLFlow, and others. The software provides examples how to build three distinct pipelines for data query and save, data engineering, and data science. These pipelines enable scalable, repeatable, and maintainable development of ML models.;false;https://github.com/nasa/ML-airport-configuration
pipeline data science language:python;fika;karthikraja95/fika;Automate Data Science Workflow from Exploratory Data Analysis to Model Deployment;false;https://github.com/karthikraja95/fika
pipeline data science language:python;ds-template;renjithbaby23/ds-template;A project template for data science projects with optimal static code analysis and CI-CD pipelines.;false;https://github.com/renjithbaby23/ds-template
pipeline data science language:python;LambdaTMDB;broepke/LambdaTMDB;How to Setup a Simple ETL Pipeline with AWS Lambda for Data Science;false;https://github.com/broepke/LambdaTMDB
pipeline data science language:python;windmill_power_prediction;SergeiAP/windmill_power_prediction;Data Science case for windmill power prediction based on weather. Based on Data Challenge of Air Liquide (@AirLiquide) and TotalEnergies(@total-sa, @Total-RD) companies in 2021. The link of the competition - https://datascience.total.com/fr/challenge/19/details#.;false;https://github.com/SergeiAP/windmill_power_prediction
pipeline data science language:python;ML-airport-departure-runway;nasa/ML-airport-departure-runway;The ML-airport-departure-runway software is developed to provide a reference implementation to serve as a research example how to train and register Machine Learning (ML) models intended for predicting departure runway assignments. The software is designed to point to databases which are not provided as part of the software release and thus this software is only intended to serve as an example of best practices. The software is built in python and leverages open-source libraries kedro, scikitlearn, MLFlow, and others. The software provides examples how to build three distinct pipelines for data query and save, data engineering, and data science. These pipelines enable scalable, repeatable, and maintainable development of ML models.;false;https://github.com/nasa/ML-airport-departure-runway
pipeline data science language:python;21_DS_Branch;1337-Artificial-Intelligence/21_DS_Branch;The piscine focuses on basic programming skills with Python and the most popular and useful data science libraries. The participants will be able to collect data with parsing, preprocess data using Pandas and SQL and build pipelines with machine learning algorithms.;false;https://github.com/1337-Artificial-Intelligence/21_DS_Branch
pipeline data science language:python;ML-airport-arrival-runway;nasa/ML-airport-arrival-runway;The ML-airport-arrival-runway software is developed to provide a reference implementation to serve as a research example how to train and register Machine Learning (ML) models intended for predicting arrival runway assignments. The software is designed to point to databases which are not provided as part of the software release and thus this software is only intended to serve as an example of best practices. The software is built in python and leverages open-source libraries kedro, scikitlearn, MLFlow, and others. The software provides examples how to build three distinct pipelines for data query and save, data engineering, and data science. These pipelines enable scalable, repeatable, and maintainable development of ML models.;false;https://github.com/nasa/ML-airport-arrival-runway
pipeline data science language:python;ML-airport-taxi-in;nasa/ML-airport-taxi-in;The ML-airport-taxi-in software is developed to provide a reference implementation to serve as a research example how to train and register Machine Learning (ML) models intended for four distinct use cases: 1) unimpeded AMA taxi in, 2) unimpeded ramp taxi in, 3) impeded AMA taxi in, and 4) impeded ramp taxi in. The software is designed to point to databases which are not provided as part of the software release and thus this software is only intended to serve as an example of best practices. The software is built in python and leverages open-source libraries kedro, scikitlearn, MLFlow, and others. The software provides examples how to build three distinct pipelines for data query and save, data engineering, and data science. These pipelines enable scalable, repeatable, and maintainable development of ML models.;false;https://github.com/nasa/ML-airport-taxi-in
pipeline data science language:python;ML-airport-estimated-ON;nasa/ML-airport-estimated-ON;The ML-airport-estimated-ON software is developed to provide a reference implementation to serve as a research example how to train and register Machine Learning (ML) models intended for predicting arrival ON time. The software is designed to point to databases which are not provided as part of the software release and thus this software is only intended to serve as an example of best practices. The software is built in python and leverages open-source libraries kedro, scikitlearn, MLFlow, and others. The software provides examples how to build three distinct pipelines for data query and save, data engineering, and data science. These pipelines enable scalable, repeatable, and maintainable development of ML models.;false;https://github.com/nasa/ML-airport-estimated-ON
pipeline data science language:python;Parkinson-Disease-Prediction;Aastha2104/Parkinson-Disease-Prediction;"Introduction  Parkinson’s Disease is the second most prevalent neurodegenerative disorder after Alzheimer’s, affecting more than 10 million people worldwide. Parkinson’s is characterized primarily by the deterioration of motor and cognitive ability. There is no single test which can be administered for diagnosis. Instead, doctors must perform a careful clinical analysis of the patient’s medical history. Unfortunately, this method of diagnosis is highly inaccurate. A study from the National Institute of Neurological Disorders finds that early diagnosis (having symptoms for 5 years or less) is only 53% accurate. This is not much better than random guessing, but an early diagnosis is critical to effective treatment. Because of these difficulties, I investigate a machine learning approach to accurately diagnose Parkinson’s, using a dataset of various speech features (a non-invasive yet characteristic tool) from the University of Oxford. Why speech features? Speech is very predictive and characteristic of Parkinson’s disease; almost every Parkinson’s patient experiences severe vocal degradation (inability to produce sustained phonations, tremor, hoarseness), so it makes sense to use voice to diagnose the disease. Voice analysis gives the added benefit of being non-invasive, inexpensive, and very easy to extract clinically. Background  Parkinson's Disease  Parkinson’s is a progressive neurodegenerative condition resulting from the death of the dopamine containing cells of the substantia nigra (which plays an important role in movement). Symptoms include: “frozen” facial features, bradykinesia (slowness of movement), akinesia (impairment of voluntary movement), tremor, and voice impairment. Typically, by the time the disease is diagnosed, 60% of nigrostriatal neurons have degenerated, and 80% of striatal dopamine have been depleted. Performance Metrics  TP = true positive, FP = false positive, TN = true negative, FN = false negative Accuracy: (TP+TN)/(P+N) Matthews Correlation Coefficient: 1=perfect, 0=random, -1=completely inaccurate Algorithms Employed  Logistic Regression (LR): Uses the sigmoid logistic equation with weights (coefficient values) and biases (constants) to model the probability of a certain class for binary classification. An output of 1 represents one class, and an output of 0 represents the other. Training the model will learn the optimal weights and biases. Linear Discriminant Analysis (LDA): Assumes that the data is Gaussian and each feature has the same variance. LDA estimates the mean and variance for each class from the training data, and then uses properties of statistics (Bayes theorem , Gaussian distribution, etc) to compute the probability of a particular instance belonging to a given class. The class with the largest probability is the prediction. k Nearest Neighbors (KNN): Makes predictions about the validation set using the entire training set. KNN makes a prediction about a new instance by searching through the entire set to find the k “closest” instances. “Closeness” is determined using a proximity measurement (Euclidean) across all features. The class that the majority of the k closest instances belong to is the class that the model predicts the new instance to be. Decision Tree (DT): Represented by a binary tree, where each root node represents an input variable and a split point, and each leaf node contains an output used to make a prediction. Neural Network (NN): Models the way the human brain makes decisions. Each neuron takes in 1+ inputs, and then uses an activation function to process the input with weights and biases to produce an output. Neurons can be arranged into layers, and multiple layers can form a network to model complex decisions. Training the network involves using the training instances to optimize the weights and biases. Naive Bayes (NB): Simplifies the calculation of probabilities by assuming that all features are independent of one another (a strong but effective assumption). Employs Bayes Theorem to calculate the probabilities that the instance to be predicted is in each class, then finds the class with the highest probability. Gradient Boost (GB): Generally used when seeking a model with very high predictive performance. Used to reduce bias and variance (“error”) by combining multiple “weak learners” (not very good models) to create a “strong learner” (high performance model). Involves 3 elements: a loss function (error function) to be optimized, a weak learner (decision tree) to make predictions, and an additive model to add trees to minimize the loss function. Gradient descent is used to minimize error after adding each tree (one by one). Engineering Goal  Produce a machine learning model to diagnose Parkinson’s disease given various features of a patient’s speech with at least 90% accuracy and/or a Matthews Correlation Coefficient of at least 0.9. Compare various algorithms and parameters to determine the best model for predicting Parkinson’s.  Dataset Description  Source: the University of Oxford 195 instances (147 subjects with Parkinson’s, 48 without Parkinson’s) 22 features (elements that are possibly characteristic of Parkinson’s, such as frequency, pitch, amplitude / period of the sound wave) 1 label (1 for Parkinson’s, 0 for no Parkinson’s) Project Pipeline  pipeline  Summary of Procedure  Split the Oxford Parkinson’s Dataset into two parts: one for training, one for validation (evaluate how well the model performs) Train each of the following algorithms with the training set: Logistic Regression, Linear Discriminant Analysis, k Nearest Neighbors, Decision Tree, Neural Network, Naive Bayes, Gradient Boost Evaluate results using the validation set Repeat for the following training set to validation set splits: 80% training / 20% validation, 75% / 25%, and 70% / 30% Repeat for a rescaled version of the dataset (scale all the numbers in the dataset to a range from 0 to 1: this helps to reduce the effect of outliers) Conduct 5 trials and average the results Data  a_o  a_r  m_o  m_r  Data Analysis  In general, the models tended to perform the best (both in terms of accuracy and Matthews Correlation Coefficient) on the rescaled dataset with a 75-25 train-test split. The two highest performing algorithms, k Nearest Neighbors and the Neural Network, both achieved an accuracy of 98%. The NN achieved a MCC of 0.96, while KNN achieved a MCC of 0.94. These figures outperform most existing literature and significantly outperform current methods of diagnosis. Conclusion and Significance  These robust results suggest that a machine learning approach can indeed be implemented to significantly improve diagnosis methods of Parkinson’s disease. Given the necessity of early diagnosis for effective treatment, my machine learning models provide a very promising alternative to the current, rather ineffective method of diagnosis. Current methods of early diagnosis are only 53% accurate, while my machine learning model produces 98% accuracy. This 45% increase is critical because an accurate, early diagnosis is needed to effectively treat the disease. Typically, by the time the disease is diagnosed, 60% of nigrostriatal neurons have degenerated, and 80% of striatal dopamine have been depleted. With an earlier diagnosis, much of this degradation could have been slowed or treated. My results are very significant because Parkinson’s affects over 10 million people worldwide who could benefit greatly from an early, accurate diagnosis. Not only is my machine learning approach more accurate in terms of diagnostic accuracy, it is also more scalable, less expensive, and therefore more accessible to people who might not have access to established medical facilities and professionals. The diagnosis is also much simpler, requiring only a 10-15 second voice recording and producing an immediate diagnosis. Future Research  Given more time and resources, I would investigate the following: Create a mobile application which would allow the user to record his/her voice, extract the necessary vocal features, and feed it into my machine learning model to diagnose Parkinson’s. Use larger datasets in conjunction with the University of Oxford dataset. Tune and improve my models even further to achieve even better results. Investigate different structures and types of neural networks. Construct a novel algorithm specifically suited for the prediction of Parkinson’s. Generalize my findings and algorithms for all types of dementia disorders, such as Alzheimer’s. References  Bind, Shubham. ""A Survey of Machine Learning Based Approaches for Parkinson Disease Prediction."" International Journal of Computer Science and Information Technologies 6 (2015): n. pag. International Journal of Computer Science and Information Technologies. 2015. Web. 8 Mar. 2017. Brooks, Megan. ""Diagnosing Parkinson's Disease Still Challenging."" Medscape Medical News. National Institute of Neurological Disorders, 31 July 2014. Web. 20 Mar. 2017. Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection', Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. BioMedical Engineering OnLine 2007, 6:23 (26 June 2007) Hashmi, Sumaiya F. ""A Machine Learning Approach to Diagnosis of Parkinson’s Disease.""Claremont Colleges Scholarship. Claremont College, 2013. Web. 10 Mar. 2017. Karplus, Abraham. ""Machine Learning Algorithms for Cancer Diagnosis."" Machine Learning Algorithms for Cancer Diagnosis (n.d.): n. pag. Mar. 2012. Web. 20 Mar. 2017. Little, Max. ""Parkinsons Data Set."" UCI Machine Learning Repository. University of Oxford, 26 June 2008. Web. 20 Feb. 2017. Ozcift, Akin, and Arif Gulten. ""Classifier Ensemble Construction with Rotation Forest to Improve Medical Diagnosis Performance of Machine Learning Algorithms."" Computer Methods and Programs in Biomedicine 104.3 (2011): 443-51. Semantic Scholar. 2011. Web. 15 Mar. 2017. ""Parkinson’s Disease Dementia."" UCI MIND. N.p., 19 Oct. 2015. Web. 17 Feb. 2017. Salvatore, C., A. Cerasa, I. Castiglioni, F. Gallivanone, A. Augimeri, M. Lopez, G. Arabia, M. Morelli, M.c. Gilardi, and A. Quattrone. ""Machine Learning on Brain MRI Data for Differential Diagnosis of Parkinson's Disease and Progressive Supranuclear Palsy.""Journal of Neuroscience Methods 222 (2014): 230-37. 2014. Web. 18 Mar. 2017. Shahbakhi, Mohammad, Danial Taheri Far, and Ehsan Tahami. ""Speech Analysis for Diagnosis of Parkinson’s Disease Using Genetic Algorithm and Support Vector Machine.""Journal of Biomedical Science and Engineering 07.04 (2014): 147-56. Scientific Research. July 2014. Web. 2 Mar. 2017. ""Speech and Communication."" Speech and Communication. Parkinson's Disease Foundation, n.d. Web. 22 Mar. 2017. Sriram, Tarigoppula V. S., M. Venkateswara Rao, G. V. Satya Narayana, and D. S. V. G. K. Kaladhar. ""Diagnosis of Parkinson Disease Using Machine Learning and Data Mining Systems from Voice Dataset."" SpringerLink. Springer, Cham, 01 Jan. 1970. Web. 17 Mar. 2017.";false;https://github.com/Aastha2104/Parkinson-Disease-Prediction
modular data pipeline language:python;datapackage-pipelines;frictionlessdata/datapackage-pipelines;Framework for processing data packages in pipelines of modular components.;false;https://github.com/frictionlessdata/datapackage-pipelines
modular data pipeline language:python;MERlin;emanuega/MERlin;MERlin is an extensible analysis pipeline applied to decoding MERFISH data;false;https://github.com/emanuega/MERlin
modular data pipeline language:python;automation;phytooracle/automation;Modular, Scalable Phenomic Data Processing Pipelines;false;https://github.com/phytooracle/automation
modular data pipeline language:python;MONSDA;jfallmann/MONSDA;MONSDA, Modular Organizer of Nextflow and Snakemake driven hts Data Analysis;false;https://github.com/jfallmann/MONSDA
modular data pipeline language:python;TP3Py;SUNYOpt/TP3Py;TP3PY: Modular Streaming Pipeline of Eye/Head Tracking Data Using Tobii Pro Glasses 3;false;https://github.com/SUNYOpt/TP3Py
modular data pipeline language:python;peppro;databio/peppro;A modular, containerized pipeline for PRO-seq data processing;false;https://github.com/databio/peppro
modular data pipeline language:python;dtflw;SoleyIo/dtflw;dtflw is a Python framework for building modular data pipelines based on Databricks dbutils.notebook API.;false;https://github.com/SoleyIo/dtflw
modular data pipeline language:python;pavlov;ntaylorwss/pavlov;A modular, composable approach to reinforcement learning with a Keras backend and a functional data pipeline.;false;https://github.com/ntaylorwss/pavlov
modular data pipeline language:python;ralsei-py;snorkysnark/ralsei-py;Build modular data pipelines running inside the postgres database;false;https://github.com/snorkysnark/ralsei-py
modular data pipeline language:python;hydra-nlp-neural-nets;shahrukhx01/hydra-nlp-neural-nets;A hydra based Natural Language Processing (NLP) pipeline boilerplate with loggers embbeded, allowing for streamlining deep learning models and accomodating experimentation while also being able to write modular scalable code. The best feature is that the code is completely parametrized via config file, which minimizes code changes when data changes etc.;false;https://github.com/shahrukhx01/hydra-nlp-neural-nets
pipeline data dashboard language:python;audiophile-e2e-pipeline;ris-tlp/audiophile-e2e-pipeline;Pipeline that extracts data from Crinacle's Headphone and InEarMonitor databases and finalizes data for a Metabase Dashboard.;false;https://github.com/ris-tlp/audiophile-e2e-pipeline
pipeline data dashboard language:python;Stock-SEC-Data-Dashboard;hrshtsharma17/Stock-SEC-Data-Dashboard;This project aims to generate an ETL pipeline for SEC EDGAR system data dashboard and analysis.;false;https://github.com/hrshtsharma17/Stock-SEC-Data-Dashboard
pipeline data dashboard language:python;udacity-data-eng-proj3;shravan-kuchkula/udacity-data-eng-proj3;Built a stream processing data pipeline to get data from disparate systems into a dashboard using Kafka as an intermediary. ;false;https://github.com/shravan-kuchkula/udacity-data-eng-proj3
pipeline data dashboard language:python;datajoint-dashboard;datajoint/datajoint-dashboard;Developing tool to build a Plotly Dash app for a DataJoint pipeline;false;https://github.com/datajoint/datajoint-dashboard
pipeline data dashboard language:python;aws-data-pipeline;ismaildawoodjee/aws-data-pipeline;A batch processing data pipeline, using AWS resources (S3, EMR, Redshift, EC2, IAM), provisioned via Terraform, and orchestrated from locally hosted Airflow containers. The end product is a Superset dashboard and a Postgres database, hosted on an EC2 instance at this address (powered down):;false;https://github.com/ismaildawoodjee/aws-data-pipeline
pipeline data dashboard language:python;youtube_data_analysis;Nupurgopali/youtube_data_analysis;Created an optimised pipeline to provide accurate data for analysis, then used snowsight (provided by Snowflake) to create a dashboard.;false;https://github.com/Nupurgopali/youtube_data_analysis
pipeline data dashboard language:python;Prescriber-ETL-data-pipeline;judeleonard/Prescriber-ETL-data-pipeline;An End-to-End ETL data pipeline that leverages pyspark parallel processing to process about 25 million rows of data coming from a SaaS application using Apache Airflow as an orchestration tool and various data warehouse technologies and finally using Apache Superset to connect to DWH for generating BI dashboards for weekly reports ;false;https://github.com/judeleonard/Prescriber-ETL-data-pipeline
pipeline data dashboard language:python;End-to-end-data-enginnerring-project;rafik-rahoui/End-to-end-data-enginnerring-project;End-to-end ELT data engineering project ;false;https://github.com/rafik-rahoui/End-to-end-data-enginnerring-project
pipeline data dashboard language:python;reddit-data-engineering;zacharyt-cs/reddit-data-engineering;An end-to-end data engineering pipeline to create a dashboard for the latest content on the r/Stocks subreddit;false;https://github.com/zacharyt-cs/reddit-data-engineering
pipeline data dashboard language:python;covid-compared;jjjchens235/covid-compared;An automated data pipeline + COVID19 comparison dashboard.;false;https://github.com/jjjchens235/covid-compared
pipeline data dashboard language:python;Data_Engineering_Projects;AuFeld/Data_Engineering_Projects;A collection of data engineering projects: data modeling, ETL pipelines, data lakes, infrastructure configuration on AWS, data warehousing, containerization, and a dashboard to monitor data pipeline KPIs ;false;https://github.com/AuFeld/Data_Engineering_Projects
pipeline data dashboard language:python;Stock_streaming_pipeline_project;Joshua-omolewa/Stock_streaming_pipeline_project;Built a real-time streaming pipeline to extract stock data, using Apache Nifi, Debezium, Kafka, and Spark Streaming. Loaded the transformed data into Glue database and created real-time dashboards using Power BI and Tableau with Athena. The pipeline is orchestrated using Airflow.;false;https://github.com/Joshua-omolewa/Stock_streaming_pipeline_project
pipeline data dashboard language:python;ETL-Mage-GCP;vinamrgrover/ETL-Mage-GCP;An ETL Pipeline built over GCP and orchestrated by Mage, which involves Extracting Data from GCS Bucket, building Dimensional Model (Star Schema), loading the Data into BigQuery and a Looker Dashboard for further analysis.;false;https://github.com/vinamrgrover/ETL-Mage-GCP
pipeline data dashboard language:python;taiko-data-pipeline;aoraki-labs/taiko-data-pipeline;Code base for taiko dashboard data pipeline.;false;https://github.com/aoraki-labs/taiko-data-pipeline
pipeline data dashboard language:python;Cryptocurrency-Data-Pipeline;jlskent/Cryptocurrency-Data-Pipeline;-Implemented a high performance data processing platform using Apache Kafka, Apache HBase, and Apache Spark to analyze cryptocurrency data.  -Developed a dashboard web app using Redis, Node.js and D3.  -Created a scalable cloud deployment environment using Docker.;false;https://github.com/jlskent/Cryptocurrency-Data-Pipeline
pipeline data dashboard language:python;Reddit_Data_Pipeline;joyceannie/Reddit_Data_Pipeline;The purpose of the project is to create a data pipeline to extract data from Reddit API and create a dashboard to analyse the data. The data is extracted from the subreddit r/Python. The data is extracted daily and uploaded to S3 buckets, and copied to Redshift. The dashboard is created using Google Data Studio.;false;https://github.com/joyceannie/Reddit_Data_Pipeline
pipeline data dashboard language:python;Tiki-Data-Analysis-V2;EdwardNgo/Tiki-Data-Analysis-V2;Pipeline for getting the tiki data to analysis some attributes and making some dashboards;false;https://github.com/EdwardNgo/Tiki-Data-Analysis-V2
pipeline data dashboard language:python;NYC-Uber-Data-Engineering-Project-with-Google-Cloud-Platform;manyuzhang1996/NYC-Uber-Data-Engineering-Project-with-Google-Cloud-Platform;Deploying ETL pipeline on cloud, analyzing on BigQuery, dashboarding on Looker Studio;false;https://github.com/manyuzhang1996/NYC-Uber-Data-Engineering-Project-with-Google-Cloud-Platform
pipeline data dashboard language:python;Real-Time-Streaming-Data-Pipeline;gborn/Real-Time-Streaming-Data-Pipeline;Real-Time Data streaming Pipeline to process and transform live events using Kafka, Spark Streaming, Hadoop, PostgresSQL on Docker. Live event metrics were monitored using a dashboard built with Django  and javascript.;false;https://github.com/gborn/Real-Time-Streaming-Data-Pipeline
pipeline data dashboard language:python;Netflix-Movie-Recommendation-System;chenxi1103/Netflix-Movie-Recommendation-System;Developed a full-stack movie recommendation system with RESTful API to provide clients with 20 recommended movies. Determine the hit rate by collecting clients’ watching data to see if they watch the recommended movies afterwards. Continuous Integration for pipeline code. Automated daily model quality evaluation and system supervision with Jinkens. Designed and built the infrastructure that can incrementally deploy new versions of recommendation service triggered by canary release and A/B testing. Integrated with feedback loops mechanism to detect potential positive or negative feedback loops to further identify potential adversarial attacks. Implemented the monitoring and detection by applying lambda architecture to combine the stream and batch processing results to detect problematic behaviors. Comprehensive data quality control on raw data received from Kafka stream, especially focus on data schema issues, missing data, and duplicated data. Monitoring Dashboard UI with D3.js. Developed the whole web server by Flask. Containerized the whole service by Docker.;false;https://github.com/chenxi1103/Netflix-Movie-Recommendation-System
pipeline data dashboard language:python;Marketing-Dashboard;Patcharanat/Marketing-Dashboard;Tools integration - Python data pipeline, SQLite database, Power BI Dashboard;false;https://github.com/Patcharanat/Marketing-Dashboard
pipeline data dashboard language:python;DFA-data-pipelines;kevanhannah/DFA-data-pipelines;A project with Airflow that powers Google Looker Studio dashboards on Digital First Assessment results and active assessor participation.;false;https://github.com/kevanhannah/DFA-data-pipelines
pipeline data dashboard language:python;bg-portfolio_etl_2_amazon;bg-portfolio/bg-portfolio_etl_2_amazon;Data pipeline for amazon shop, ETL, scraper, docker, python. Airflow, MongoDB, Selenium Grid, Streamlit Dashboard.;false;https://github.com/bg-portfolio/bg-portfolio_etl_2_amazon
pipeline data dashboard language:python;Austin_Driver_Score_Predictor;cedoula/Austin_Driver_Score_Predictor;Used Python Scikit-Learn to analyze Austin car crash data from 2018 to 2020 and created an interactive dashboard using a Random Forest Classifier algorithm to calculate a driver score from user features.;false;https://github.com/cedoula/Austin_Driver_Score_Predictor
pipeline data dashboard language:python;news-headlines;dhruvi-9/news-headlines;Pipeline that extracts data from sites for a dashboard.;false;https://github.com/dhruvi-9/news-headlines
pipeline data dashboard language:python;velib_v1;kaoutaar/velib_v1;end to end data engineering project;false;https://github.com/kaoutaar/velib_v1
pipeline data dashboard language:python;aws-dashboard-demogo-prime;cdrhim/aws-dashboard-demogo-prime;Data pipeline and security SIEM construction, inspired by AWS DemoGo Prime Day.;false;https://github.com/cdrhim/aws-dashboard-demogo-prime
pipeline data dashboard language:python;weather_data_pipeline;James-Wachuka/weather_data_pipeline;This is a PySpark-based data pipeline that fetches weather data for a few cities, performs some basic processing and transformation on the data, and then writes the processed data to a Google Cloud Storage bucket and a BigQuery table.The data is then viewed in a looker dashboard ;false;https://github.com/James-Wachuka/weather_data_pipeline
pipeline data dashboard language:python;Viagens-Dashboard;elvinmatheus/Viagens-Dashboard;Um pipeline de dados que baixa dados do Portal da Transparência e os carrega para um Data Warehouse ;false;https://github.com/elvinmatheus/Viagens-Dashboard
pipeline data dashboard language:python;Reddit-ELT-Pipeline;BWalliz/Reddit-ELT-Pipeline;"The pipeline extracts data from the /r/DataEngineering API and exports a csv file to S3. The most recent file is ingested into Redshift and subsequently transformed with dbt. Airflow was used for orchestration and hosted locally with docker-compose; The output is a PowerBI dashboard that is connected to Redshift.";false;https://github.com/BWalliz/Reddit-ELT-Pipeline
pipeline data dashboard language:python;Data-Streaming-Using-Kafka;ruslanmv/Data-Streaming-Using-Kafka;Built a stream processing data pipeline to get data from disparate systems into a dashboard using Kafka as an intermediary.;false;https://github.com/ruslanmv/Data-Streaming-Using-Kafka
pipeline data dashboard language:python;uber-data-engineering-project;naman-raturi/uber-data-engineering-project;" Analyzed the Uber dataset, created an entire data pipeline using Google Cloud Platform (GCP), and performed ETL operations using MAGE-AI. Additionally,  developed a dynamic dashboard using LOOKER Studio. Tools &  Languages used :GCP Storage, Python, Compute Instance, Mage Data Pipeline Tool, BigQuery, and Looker Studio.";false;https://github.com/naman-raturi/uber-data-engineering-project
pipeline data dashboard language:python;ADF-Databricks-Covid19;sam-url/ADF-Databricks-Covid19;This repo contains project demo for ETL pipelines and covid dashboard using ADF as orchestration tool, storage as data lake, Databricks notebook as codebase and Power BI for dashboarding purpose.;false;https://github.com/sam-url/ADF-Databricks-Covid19
pipeline data dashboard language:python;Yelp-Data-Engineeering;Ezuniga13/Yelp-Data-Engineeering;Fully automated data pipeline using Python, Yelp API's, AWS RDS, s3 buckets, Docker and Streamlit to create a 3D interactive dashboard.;false;https://github.com/Ezuniga13/Yelp-Data-Engineeering
pipeline data dashboard language:python;sentiment-analysis-platform;tspannhw/sentiment-analysis-platform;An all-in-one content platform (currently serving Twitter tweets). Comes complete with a sentiment analysis data pipeline, an API to query analyzed content and (in the future) a front-end dashboard.;false;https://github.com/tspannhw/sentiment-analysis-platform
pipeline data dashboard language:python;gaming-nonrealtime-analytics;Azure-Samples/gaming-nonrealtime-analytics;This reference architecture represents a simple analytics pipeline that you can build on Azure. It can be leveraged when you won't be tracking data that requires real-time analysis and instead you just plan to do review sessions of the data every now and then (daily, weekly, bi-weekly, monthly). The presentation layer is a dashboard that you will be able to customize at will. You could use this while you are developing your game and in production.;false;https://github.com/Azure-Samples/gaming-nonrealtime-analytics
pipeline data dashboard language:python;Docker_VPS_App;cogentdom/Docker_VPS_App;This project scrapes data of stocks and feeds them down a pipeline that trains an ML model and displays it as a dashboard using Streamlit. This Streamlit app is then containerized using docker and deployed to a virtual machine on AWS using EC2. This EC2 instance is then accessed from a custom domain name that is being hosted on a Cloudflare DNS server.;false;https://github.com/cogentdom/Docker_VPS_App
modular data pipeline;pepatac;databio/pepatac;A modular, containerized pipeline for ATAC-seq data processing;false;https://github.com/databio/pepatac
modular data pipeline;TomoBEAR;KudryashevLab/TomoBEAR;TomoBEAR is a configurable and customizable modular pipeline for streamlined processing of cryo-electron tomographic data for subtomogram averaging.;false;https://github.com/KudryashevLab/TomoBEAR
modular data pipeline;bcmhnc-toolkit;rosman83/bcmhnc-toolkit;Modular toolkit for data analysis pipelines used at Baylor⚕️;false;https://github.com/rosman83/bcmhnc-toolkit
modular data pipeline;rbforwarder;redBorder/rbforwarder;Extensible, modular and easy to use tool for process data on a pipeline;false;https://github.com/redBorder/rbforwarder
modular data pipeline;snoke-builder-viz;EQWorks/snoke-builder-viz;Externalized UI component for modular data pipeline builder (v3) ;false;https://github.com/EQWorks/snoke-builder-viz
modular data pipeline;XTL;brendon-ng/XTL;XTL - An Extensible Extract-Transform-Load (ETL) framework that streamlines data transfers between big data systems using a simple JSON configuration-based interface. XTL minimizes the programming needed for data transfer and processing, by providing a configuration-based modular and extensible structure for common data pipeline tasks.;false;https://github.com/brendon-ng/XTL
modular data pipeline;tangara-pipeline;sebaxtian/tangara-pipeline;Tangara Pipeline uses Kedro Framework for creating reproducible, maintainable, and modular data science code. Data sources are provided from Air Quality sensors in Cali, Colombia. https://tangara.chis.pa/;false;https://github.com/sebaxtian/tangara-pipeline
modular data pipeline;metaXplore;hseabolt/metaXplore;A modular bioinformatics pipeline to conduct basic, rapid data exploration of metagenomic sequencing data at a glance.;false;https://github.com/hseabolt/metaXplore
modular data pipeline;covid19_vaccination_analysis;brooksdonald/covid19_vaccination_analysis;The COVID-19 Vaccination Analysis Pipeline (CVAP) is a modular analytical pipeline designed to consolidate, analyze, and disseminate COVID-19 vaccine implementation data across all countries, areas, and territories for use by the global community.;false;https://github.com/brooksdonald/covid19_vaccination_analysis
modular data pipeline;Dunya;AlainDaccache/Dunya;Modular infrastructure for the Data Engineering, Data Science, and ML Engineering lifecycle. Serves as a generic baseline to kickstart Artificial and Business Intelligence pipelines with industry practices.;false;https://github.com/AlainDaccache/Dunya
modular data pipeline;adfa2;djdprogramming/adfa2;# David's Personal Roadmap to Learning Data Science #### Based on the article [Learn Data Science for free in 2021](https://www.kdnuggets.com/2021/01/learn-data-science-free-2021.html) from KDnuggets. Some additions have been made. ###### I'm new to data science and programming. Some areas of study in this roadmap may be researched to a point of redundancy while materials for other topics could be seriously lacking. As I progress through this learning path, I'll be able to gauge which areas need more (or less) focus and will add and remove resources as needed.  ## Schoolwork ##### Required readings for my Data Science classes. - [ ] [Doing Data Science: Straight Talk from the Frontline](https://www.amazon.com/Doing-Data-Science-Straight-Frontline/dp/1449358659) by Cathy O'Neil & Rachel Schutt   - [ ] 1. Introduction: What is Data Science?   - [ ] 2. Statistical Inference, Exploratory Data Analysis, and the Data Science Process   - [ ] 3. Algorithms   - [ ] 4. Spam Filters, Naive Bayes, and Wrangling   - [ ] 5. Logistic Regression   - [ ] 6. Time Stamps and Financial Modeling   - [ ] 7. Extracting Meaning from Data   - [ ] 8. Recommendation Engines: Building a User-Facing Data Product at Scale   - [ ] 9. Data Visualization and Fraud Detection   - [ ] 10. Social Networks and Data Journalism   - [ ] 11. Causality   - [ ] 12. Epidemiology   - [ ] 13. Lessons Learned from Data Competitions: Data Leakage and Model Evaluation   - [ ] 14. Data Engineering: MapReduce, Pregel, and Hadoop   - [ ] 15. The Students Speak   - [ ] 16. Next-Generation Data Scientists, Hubris, and Ethics   - [ ] [Practical Statistics for Data Scientists: 50 Essential Concepts](https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential/dp/149207294X/ref=sr_1_1?dchild=1&keywords=Practical+Statistics+for+Data+Scientists&qid=1609991269&s=books&sr=1-1) by Peter Bruce, Andrew Bruce & Peter Gedeck   - [ ] 1. Exploratory Data Analysis   - [ ] 2. Data and Sampling Distributions   - [ ] 3. Statistical Experiments and Significance Testing   - [ ] 4. Regression and Prediction   - [ ] 5. Classification   - [ ] 6. Statistical Machine Learning   - [ ] 7. Unsupervised Learning  ## Programming Skills ##### Learn programming basics. - [ ] [Python 3 Basics Tutorial Series](https://www.youtube.com/playlist?list=PLQVvvaa0QuDe8XSftW-RAxdo6OmaeL85M) by sentdex   - [ ] 1. Python 3 Programming Tutorial: Why Python 3? Python 2 vs Python 3 `7:36`   - [ ] 2. Python 3 Programming Tutorial: Installing Python 3 - How to Install Both Python 2 and Python 3 `15:47`   - [ ] 3. Python 3 Programming Tutorial: Print Function and Strings `9:31`   - [ ] 4. Python 3 Programming Tutorial: Math `4:49`   - [ ] 5. Python 3 Programming Tutorial: Variables `4:26`   - [ ] 6. Python 3 Programming Tutorial: While Loop `5:55`   - [ ] 7. Python 3 Programming Tutorial: For Loop `9:05`   - [ ] 8. Python 3 Programming Tutorial: If Statement `4:54`   - [ ] 9. Python 3 Programming Tutorial: If Else `3:20`   - [ ] 10. Python 3 Programming Tutorial: If Elif Else `4:19`   - [ ] 11. Python 3 Programming Tutorial: Functions `3:05`   - [ ] 12. Python 3 Programming Tutorial: Function Parameters `4:00`   - [ ] 13. Python 3 Programming Tutorial: Function Parameter Defaults `6:06`   - [ ] 14. Python 3 Programming Tutorial: Global and Local Variables `6:31`   - [ ] 15. Python 3 Programming Tutorial: Installing Modules `7:44`   - [ ] 16. Python 3 Programming Tutorial: How to Download and Install Python Packages and Modules with Pip `8:32`   - [ ] 17. Python 3 Programming Tutorial: Common Errors `4:49`   - [ ] 18. Python 3 Programming Tutorial: Writing to File `3:35`   - [ ] 19. Python 3 Programming Tutorial: Appending Files `2:42`   - [ ] 20. Python 3 Programming Tutorial: Read from a File `1:49`   - [ ] 21. Python 3 Programming Tutorial: Classes `4:56`   - [ ] 22. Python 3 Programming Tutorial: Frequently Asked Questions `5:33`   - [ ] 23. Python 3 Programming Tutorial: Getting User Input `1:43`   - [ ] 24. Python 3 Programming Tutorial: Statistics (Mean, Standard Deviation) `2:36`   - [ ] 25. Python 3 Programming Tutorial: Module Import Syntax `5:31`   - [ ] 26. Python 3 Programming Tutorial: Making Modules `4:58`   - [ ] 27. Python 3 Programming Tutorial: Lists and Tuples `5:51`   - [ ] 28. Python 3 Programming Tutorial: List Manipulation `9:35`   - [ ] 29. Python 3 Programming Tutorial: Multi-Dimensional List `5:45`   - [ ] 30. Python 3 Programming Tutorial: Reading from a CSV Spreadsheet `9:24`   - [ ] 31. Python 3 Programming Tutorial: Try and Except Error Handlings `7:04`   - [ ] 32. Python 3 Programming Tutorial: Multi-Line Print `3:19`   - [ ] 33. Python 3 Programming Tutorial: Dictionaries `7:11`   - [ ] 34. Python 3 Programming Tutorial: Built-in Functions `10:58`   - [ ] 35. Python 3 Programming Tutorial: OS Module `5:01`   - [ ] 36. Python 3 Programming Tutorial: Sys Module `11:00`   - [ ] 37. Python 3 Programming Tutorial: urllib Module `24:04`   - [ ] 38. Python 3 Programming Tutorial: Regular Expressions/Regex with re `19:58`   - [ ] 39. Python 3 Programming Tutorial: Parsing Websites with re and urllib `7:29`   - [ ] 40. Python 3 Programming Tutorial: Tkinter Module Making Windows `8:03`   - [ ] 41. Python 3 Programming Tutorial: Tkinter Adding Buttons `6:29`   - [ ] 42. Python 3 Programming Tutorial: Tkinter Event Handling `5:40`   - [ ] 43. Python 3 Programming Tutorial: Tkinter Menu Bar `10:25`   - [ ] 44. Python 3 Programming Tutorial: Tkinter Adding Images and Text `11:59`   - [ ] 45. Python 3 Programming Tutorial: Threading Module `18:43`   - [ ] 46. Python 3 Programming Tutorial: cx_freeze Python to .exe `12:08`   - [ ] 47. Python 3 Programming Tutorial: Subprocess Module `13:17`   - [ ] 48. Python 3 Programming Tutorial: Matplotlib Graphing Intro `10:25`   - [ ] 49. Python 3 Programming Tutorial: Matplotlib Labels and Titles `5:03`   - [ ] 50. Python 3 Programming Tutorial: Matplotlib Styles `10:38`   - [ ] 51. Python 3 Programming Tutorial: Matplotlib Legends `4:07`   - [ ] 52. Python 3 Programming Tutorial: Scatter Plots and Bar Charts `6:38`   - [ ] 53. Python 3 Programming Tutorial: Matplotlib Plotting from a CSV `7:21`   - [ ] 54. Python 3 Programming Tutorial: ftplib FTP Transfers Python `8:47`   - [ ] 55. Python 3 Programming Tutorial: Sockets Intro `10:48`   - [ ] 56. Python 3 Programming Tutorial: Sockets Simple Port Scanner `5:08`   - [ ] 57. Python 3 Programming Tutorial: Threaded Port Scanner `9:36`   - [ ] 58. Python 3 Programming Tutorial: Sockets Binding and Listening `5:53`   - [ ] 59. Python 3 Programming Tutorial: Sockets Client Server System `10:27` - [ ] [Intermediate Python Programming](https://www.youtube.com/playlist?list=PLQVvvaa0QuDfju7ADVp5W1GF9jVhjbX-_) by sentdex   - [ ] 1. Intermediate Python Programming: Introduction `7:48`   - [ ] 2. Intermediate Python Programming: String Concatenation and Formatting `13:40`   - [ ] 3. Intermediate Python Programming: Argparse for CLI `10:49`   - [ ] 4. Intermediate Python Programming: List Comprehension and Generator Expressions `6:52`   - [ ] 5. Intermediate Python Programming: More on List Comp and Generators `15:28`   - [ ] 6. Intermediate Python Programming: Timeit Module `11:28`   - [ ] 7. Intermediate Python Programming: Enumerate `4:48`   - [ ] 8. Intermediate Python Programming: Zip `7:23`   - [ ] 9. Intermediate Python Programming: Writing Our Own Generator `11:08`   - [ ] 10. Intermediate Python Programming: Multiprocessing `11:30`   - [ ] 11. Intermediate Python Programming: Getting Returned Values from Processes `4:22`   - [ ] 12. Intermediate Python Programming: Multiprocessing Spider Example `24:18`   - [ ] 13. Intermediate Python Programming: Object Oriented Programming Introductions `11:35`   - [ ] 14. Intermediate Python Programming: Creating an Environment for Our Project `11:49`   - [ ] 15. Intermediate Python Programming: Many Blob Objects `8:30`   - [ ] 16. Intermediate Python Programming: Object Modularity Thoughts `16:41`   - [ ] 17. Intermediate Python Programming: OOP Inheritance `10:17`   - [ ] 18. Intermediate Python Programming: Decorators `8:50`   - [ ] 19. Intermediate Python Programming: Operator Overloading `10:19`   - [ ] 20. Intermediate Python Programming: Detecting Collisions `15:20`   - [ ] 21. Intermediate Python Programming: Special Methods, OOP, Iteration `13:30`   - [ ] 22. Intermediate Python Programming: Logging `15:00`   - [ ] 23. Intermediate Python Programming: Error Handling `6:11`   - [ ] 24. Intermediate Python Programming: --str-- and --repr-- `11:32`   - [ ] 25. Intermediate Python Programming: Args and Kwargs `11:58`   - [ ] 26. Intermediate Python Programming: Asyncio - Asynchronous Programming with Coroutines `28:37` - [ ] [2021 Complete Python Bootcamp From Zero to Hero in Python](https://www.udemy.com/course/complete-python-bootcamp/) by Jose Portilla   - [ ] 1. Course Overview   - [ ] 2. Python Setup   - [ ] 3. Python Object and Data Structure Basics   - [ ] 4. Python Comparison Operators   - [ ] 5. Python Statements   - [ ] 6. Methods and Functions   - [ ] 7. Milestone Project 1   - [ ] 8. Object Oriented Programming   - [ ] 9. Modules and Packages   - [ ] 10. Errors and Exceptions Handlings   - [ ] 11. Milestone Project 2   - [ ] 12. Python Decorators   - [ ] 13. Python Generators   - [ ] 14. Advanced Python Modules   - [ ] 15. Web Scraping with Python   - [ ] 16. Working with Images with Python   - [ ] 17. Working with PDFs and Spreadsheet CSV Files   - [ ] 18. Emails with Python   - [ ] 19. Final Capstone Python Project   - [ ] 20. Advanced Python Objects and Data Structures   - [ ] 21. Bonus Material - Introduction to GUIs - [ ] Build the 5 projects listed in the [5 Intermediate Python Projects](https://www.youtube.com/watch?v=o5sb8ehRSYA&ab_channel=TechWithTim) video by Tech With Tim   - [ ] 1. Build a Website with Django/Flask   - [ ] 2. Use a WebScraper   - [ ] 3. Create a Game with PyGame   - [ ] 4. Build a GUI with Tkinter/PyQt5   - [ ] 5. Robotics/Raspberry Pi Project    ## Data Analysis and Visualization ##### Learn NumPy, Pandas and Matplotlib. - [ ] [Python NumPy Tutorial for Beginners](https://www.youtube.com/watch?v=QUT1VHiLmmI&ab_channel=freeCodeCamp.org) by freeCodeCamp.org `58:09` - [ ] Read the [Introduction to NumPy](https://jakevdp.github.io/PythonDataScienceHandbook/02.00-introduction-to-numpy.html) chapter from the Python Data Science Handbook by Jake VanderPlas   - [ ] 1. Introduction to NumPy   - [ ] 2. Understanding Data Types in Python   - [ ] 3. The Basics of NumPy Arrays   - [ ] 4. Computation on NumPy Arrays: Universal Functions   - [ ] 5. Aggregations: Min, Max, and Everything in Between   - [ ] 6. Computation on Arrays: Broadcasting   - [ ] 7. Comparisons, Masks, and Boolean Logic   - [ ] 8. Fancy Indexing   - [ ] 9. Sorting Arrays   - [ ] 10. Structured Data: NumPy's Structured Arrays - [ ] [Pandas Tutorials](https://www.youtube.com/playlist?list=PL-osiE80TeTsWmV9i9c58mdDCSskIFdDS) by Corey Schafer   - [ ] 1. Python Pandas Tutorial: Getting Started with Data Analysis - Installation and Loading Data `23:01`   - [ ] 2. Python Pandas Tutorial: DataFrame and Series Basics - Selecting Rows and Columns `33:35`   - [ ] 3. Python Pandas Tutorial: Indexes - How to Set, Reset, and Use Indexes `17:27`   - [ ] 4. Python Pandas Tutorial: Filtering - Using Conditionals to Filter Rows and Columns `23:04`   - [ ] 5. Python Pandas Tutorial: Updating Rows and Columns - Modifying Data within DataFrames `40:03`   - [ ] 6. Python Pandas Tutorial: Add/Remove Rows and Columns from DataFrames `16:55`   - [ ] 7. Python Pandas Tutorial: Sorting Data `15:40`   - [ ] 8. Python Pandas Tutorial: Grouping and Aggregating - Analyzing and Exploring Your Data `49:06`   - [ ] 9. Python Pandas Tutorial: Cleaning Data - Casting Data Types and Handling Missing Values `31:54`   - [ ] 10. Python Pandas Tutorial: Working with Dates and Time Series Data `35:41`   - [ ] 11. Python Pandas Tutorial: Reading/Writing Data to Different Sources - Excel, JSON, SQL, Etc. `32:45` - [ ] Read the [Data Manipulation with Pandas](https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html) chapter from the Python Data Science Handbook by Jake VanderPlas   - [ ] 1. Data Manipulation with Pandas   - [ ] 2. Introducing Pandas Objects   - [ ] 3. Data Indexing and Selection   - [ ] 4. Operating on Data in Pandas   - [ ] 5. Handling Missing Data   - [ ] 6. Hierarchical Indexing   - [ ] 7. Combining Datasets: Concat and Append   - [ ] 8. Combining Datasets: Merge and Join   - [ ] 9. Aggregation and Grouping   - [ ] 10. Pivot Tables   - [ ] 11. Vectorized String Operations   - [ ] 12. Working with Time Series   - [ ] 13. High-Performance Pandas: eval() and query() - [ ] [Matplotlib Tutorials](https://www.youtube.com/playlist?list=PL-osiE80TeTvipOqomVEeZ1HRrcEvtZB_) by Corey Schafer   - [ ] 1. Matplotlib Tutorial: Creating and Customizing Our First Plots `35:01`   - [ ] 2. Matplotlib Tutorial: Bar Charts and Analyzing Data from CSVs `34:26`   - [ ] 3. Matplotlib Tutorial: Pie Charts `17:02`   - [ ] 4. Matplotlib Tutorial: Stack Plots `14:49`   - [ ] 5. Matplotlib Tutorial: Filling Area on Line Plots `15:18`   - [ ] 6. Matplotlib Tutorial: Histograms `16:36`   - [ ] 7. Matplotlib Tutorial: Scatter Plots `21:24`   - [ ] 8. Matplotlib Tutorial: Plotting Time Series Data `17:09`   - [ ] 9. Matplotlib Tutorial: Plotting Live Data in Real-Time `20:34`   - [ ] 10. Matplotlib Tutorial: Subplots `21:22` - [ ] Read the [Visualization with Matplotlib](https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html) chapter from the Python Data Science Handbook by Jake VanderPlas   - [ ] 1. Visualization with Matplotlib   - [ ] 2. Simple Line Plots   - [ ] 3. Simple Scatter Plots   - [ ] 4. Visualizing Errors   - [ ] 5. Density and Contour Plots   - [ ] 6. Histograms, Binnings, and Density   - [ ] 7. Customizing Plot Legends   - [ ] 8. Customizing Colorbars   - [ ] 9. Multiple Subplots   - [ ] 10. Text and Annotation   - [ ] 11. Customizing Ticks   - [ ] 12. Customizing Matplotlib: Configurations and Stylesheets   - [ ] 13. Three-Dimensional Plotting in Matplotlib   - [ ] 14. Geographic Data with Basemap   - [ ] 15. Visualization with Seaborn - [ ] [Python for Data Science - Course for Beginners (Learn Python, Pandas, NumPy, Matplotlib)](https://www.youtube.com/watch?v=LHBE6Q9XlzI&t=2s&ab_channel=freeCodeCamp.org) by freeCodeCamp.org `12:19:51`  ## Data Preprocessing ##### Learn the basics of data preprocessing. - [ ] [Data Cleaning](https://www.kaggle.com/learn/data-cleaning) by Kaggle   - [ ] 1. Handling Missing Values   - [ ] 2. Scaling and Normalization   - [ ] 3. Parsing Dates   - [ ] 4. Character Encodings   - [ ] 5. Inconsistent Data Entry - [ ] Do the [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition by Kaggle - [ ] Do the [Housing Prices](https://www.kaggle.com/c/home-data-for-ml-course) competition by Kaggle - [ ] [Feature Engineering](https://www.kaggle.com/learn/feature-engineering) by Kaggle   - [ ] 1. Baseline Model   - [ ] 2. Categorical Encodings   - [ ] 3. Feature Generation   - [ ] 4. Feature Selection    ## Databases ##### Learn about databases. - [ ] [Intro to SQL](https://www.kaggle.com/learn/intro-to-sql) by Kaggle   - [ ] 1. Getting Started with SQL and BigQuery   - [ ] 2. Select, From & Where   - [ ] 3. Group By, Having & Count   - [ ] 4. Order By   - [ ] 5. As & With   - [ ] 6. Joining Data - [ ] [Advanced SQL](https://www.kaggle.com/learn/advanced-sql) by Kaggle   - [ ] 1. JOINs and UNIONs   - [ ] 2. Analytic Functions   - [ ] 3. Nested and Repeated Data   - [ ] 4. Writing Efficient Queries - [ ] [MongoDB with Python Crash Course - Tutorial for Beginners](https://www.youtube.com/watch?v=E-1xI85Zog8&ab_channel=freeCodeCamp.org) by freeCodeCamp.org `1:57:33`  ## Machine Learning ##### Taking our first steps into the world of ML. - [ ] [Machine Learning](https://www.coursera.org/learn/machine-learning#syllabus) by Andrew Ng (skipping the MATLAB section)   - [ ] 1. Introduction   - [ ] 2. Linear Regression with One Variable   - [ ] 3. Linear Algebra Review   - [ ] 4. Linear Regression with Multiple Variables   - [ ] 5. Logistic Regression   - [ ] 6. Regularization   - [ ] 7. Neural Networks: Representation   - [ ] 8. Neural Networks: Learning   - [ ] 9: Advice for Applying Machine Learning   - [ ] 10. Machine Learning System Design   - [ ] 11. Support Vector Machines   - [ ] 12. Unsupervised Learning   - [ ] 13. Dimensionality Reduction   - [ ] 14. Anomaly Detection   - [ ] 15. Recommender Systems   - [ ] 16. Large Scale Machine Learning   - [ ] 17. Application Example: Photo OCR - [ ] [Coursera Machine Learning MOOC by Andrew Ng Python Programming Assignments](https://github.com/dibgerge/ml-coursera-python-assignments)   - [ ] Exercise 1   - [ ] Exercise 2   - [ ] Exercise 3   - [ ] Exercise 4   - [ ] Exercise 5   - [ ] Exercise 6   - [ ] Exercise 7   - [ ] Exercise 8 - [ ] Do any [Kaggle](https://www.kaggle.com/) competition - [ ] [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) by Kaggle   - [ ] 1. Introduction   - [ ] 2. Missing Values   - [ ] 3. Categorical Variables   - [ ] 4. Pipelines   - [ ] 5. Cross-Validation   - [ ] 6. XGBoost   - [ ] 7. Data Leakage    ## Linear Algebra and Statistics  ##### Learn linear algebra and statistics. - [ ] [Linear Algebra](https://www.khanacademy.org/math/linear-algebra) on Khan Academy   - [ ] 1. Vectors and Spaces   - [ ] 2. Matrix Transformations   - [ ] 3. Alternate Coordinate Systems (Bases) - [ ] [Linear Algebra](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm) on MIT OpenCourseWare   - [ ] Problem Set 1   - [ ] Problem Set 2   - [ ] Problem Set 3   - [ ] Problem Set 4   - [ ] Problem Set 5   - [ ] Problem Set 6   - [ ] Problem Set 7   - [ ] Problem Set 8   - [ ] Problem Set 9   - [ ] Problem Set 10   - [ ] Exam 1   - [ ] Exam 2   - [ ] Exam 3   - [ ] Final Exam - [ ] [Statistics and Probability](https://www.khanacademy.org/math/statistics-probability) on Khan Academy   - [ ] 1. Analyzing Categorical Data   - [ ] 2. Displaying and Comparing Quantitative Data   - [ ] 3. Summarizing Quantitative Data   - [ ] 4. Modeling Data Distributions   - [ ] 5. Exploring Bivariate Numerical Data   - [ ] 6. Study Design   - [ ] 7. Probability   - [ ] 8. Counting, Permutations, and Combinations   - [ ] 9. Random Variables   - [ ] 10. Sampling Distributions   - [ ] 11. Confidence Intervals   - [ ] 12. Significance Tests (Hypothesis Testing)   - [ ] 13. Two-Sample Inference for the Difference Between Groups   - [ ] 14. Inference for Categorical Data (Chi-Square Tests)   - [ ] 15. Advanced Regression (Inference and Transforming)   - [ ] 16. Analysis of Variance (ANOVA) - [ ] [Introduction to Probability and Statistics](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/index.htm) on MIT OpenCourseWare   - [ ] Problem Set 1   - [ ] Problem Set 2   - [ ] Problem Set 3   - [ ] Problem Set 4   - [ ] Problem Set 5   - [ ] Problem Set 6   - [ ] Problem Set 7   - [ ] Problem Set 8   - [ ] Problem Set 9   - [ ] Exam 1 Practice Questions I   - [ ] Exam 1 Practice Questions II   - [ ] Exam 1 Practice Questions: Long List   - [ ] Exam 1   - [ ] Exam 2 Practice Questions   - [ ] Exam 2   - [ ] Final Exam Practice Questions   - [ ] Final Exam - [ ] [Deep Learning Book](https://www.deeplearningbook.org/) by Ian Goodfellow, Yoshua Bengio & Aaron Courville   - [ ] 1. Introduction   - [ ] 2. Linear Algebra   - [ ] 3. Probability and Information Theory   - [ ] 4. Numerical Computation   - [ ] 5. Machine Learning Basics   - [ ] 6. Deep Feedforward Networks   - [ ] 7. Regularization for Deep Learning   - [ ] 8. Optimization for Training Deep Models   - [ ] 9. Convolutional Networks   - [ ] 10. Sequence Modeling: Recurrent and Recursive Nets   - [ ] 11. Practical Methodology   - [ ] 12. Applications   - [ ] 13. Linear Factor Models   - [ ] 14. Autoencoders   - [ ] 15. Representation Learning   - [ ] 16. Structured Probabilistic Models for Deep Learning   - [ ] 17. Monte Carlo Methods   - [ ] 18. Confronting the Partition Function   - [ ] 19. Approximate Inference   - [ ] 20. Deep Generative Models    ## Deep Learning ##### Learning about deep learning. - [ ] [Practical Deep Learning for Coders](https://course.fast.ai/) by fast.ai   - [ ] Lesson 1   - [ ] Lesson 2   - [ ] Lesson 3   - [ ] Lesson 4   - [ ] Lesson 5   - [ ] Lesson 6   - [ ] Lesson 7   - [ ] Lesson 8 - [ ] [Part 2: Deep Learning from the Foundations](https://course19.fast.ai/part2) by fast.ai   - [ ] Lesson 1   - [ ] Lesson 2   - [ ] Lesson 3   - [ ] Lesson 4   - [ ] Lesson 5   - [ ] Lesson 6   - [ ] Lesson 7   - [ ] Lesson 8   - [ ] Lesson 9   - [ ] Lesson 10   - [ ] Lesson 11   - [ ] Lesson 12   - [ ] Lesson 13   - [ ] Lesson 14 - [ ] [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) by Andrew Ng   - [ ] Course 1: Neural Networks and Deep Learning     - [ ] 1. Introduction to Deep Learning     - [ ] 2. Neural Network Basics     - [ ] 3. Shallow Neural Networks     - [ ] 4. Deep Neural Networks   - [ ] Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization     - [ ] 1. Practical Aspects of Deep Learning     - [ ] 2. Optimization Algorithms     - [ ] 3. Hyperparameter tuning, Batch Normalization and Programming Frameworks   - [ ] Course 3: Structuring Machine Learning Projects     - [ ] 1. ML Strategy (1)     - [ ] 2. ML Strategy (2)   - [ ] Course 4: Convolutional Neural Networks     - [ ] 1. Foundations of Convolutional Neural Networks     - [ ] 2. Deep Convolutional Models: Case Studies     - [ ] 3. Object Detection     - [ ] 4. Special applications: Face recognition & Neural style transfer   - [ ] Course 5: Sequence Models     - [ ] 1. Recurrent Neural Networks     - [ ] 2. Natural Language Processing & Word Embeddings     - [ ] 3. Sequence Models & Attention Mechanism - [ ] [DeepLearning.AI TensorFlow Developer Professional Certificate](https://www.coursera.org/professional-certificates/tensorflow-in-practice?) by Laurence Moroney   - [ ] Course 1: Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning     - [ ] 1. A New Programming Paradigm     - [ ] 2. Introduction to Computer Vision     - [ ] 3. Enhancing Vision with Convolutional Neural Networks     - [ ] 4. Using Real-World Images   - [ ] Course 2: Convolutional Neural Networks in TensorFlow     - [ ] 1. Exploring a Larger Dataset     - [ ] 2. Augmentation: A Technique to Avoid Overfitting     - [ ] 3. Transfer Learning     - [ ] 4. Multiclass Classifications   - [ ] Course 3: Natural Language Processing in TensorFlow     - [ ] 1. Sentiment in Text     - [ ] 2. Word Embeddings     - [ ] 3. Sequence Models     - [ ] 4. Sequence Models and Literature   - [ ] Course 4: Sequences, Time Series and Prediction     - [ ] 1. Sequences and Prediction     - [ ] 2. Deep Neural Networks for Time Series     - [ ] 3. Recurrent Neural Networks for Time Series     - [ ] 4. Real-World Time Series Data      ##  Cloud for Model Deployment ##### Learn how to build, train, test, and deploy a machine learning model on AWS. - [ ] [AWS Machine Learning Specialty](https://www.youtube.com/playlist?list=PLEF5xKHm-3ZNDvdJpMCLu9xa1oDNvAmMr) by Amazon   - [ ] 1. AWS Training and Certification: Machine Learning `1:31`   - [ ] 2. Build, Train and Deploy Machine Learning Models on AWS with Amazon SageMaker - AWS Online `35:51`   - [ ] 3. AWS re:Invent 2018: Leadership Session: Machine Learning (AIM202-L) `58:01`   - [ ] 4. Machine Learning Models with TensorFlow Using Amazon SageMaker - AWS Online Tech Talks `40:16`   - [ ] 5. AWS re:Invent 2018: Build & Deploy ML Models Quickly & Easily with Amazon SageMaker `57:53`   - [ ] 6. AWS re:Invent 2018: CI/CD for Your Machine Learning Pipeline with Amazon SageMaker `57:13`   - [ ] 7. AWS Berlin Summit 2018 - Building and Running Your First ML Application on Amazon SageMaker `52:54`   - [ ] 8. Predictive Analytics with Amazon SageMaker `1:03:29`   - [ ] 9. AWS re:Invent 2018: AI/ML with Data Lakes: Counterintuitive Consumer Insights in Retail `1:00:10`   - [ ] 10. AWS re:Invent 2018: Industrialize Machine Learning Using CI/CD Techniques (FSV304-i) `45:34`   - [ ] 11. AWS re:Invent 2018: Driving Machine Learning and Analytics Use Cases with AWS Storage (STG302) `40:16`   - [ ] 12. AWS re:Invent 2018: Deep Learning Applications Using TensorFlow (AIM401-R) `1:02:29`   - [ ] 13. AWS re:Invent 2017: Machine Learning State of the Union (MCL210) `1:00:55`   - [ ] 14. AWS re:Invent 2017: Containerized Machine Learning on AWS (CON309) `1:03:21`   - [ ] 15. AWS re:Invent 2017: Introduction to Deep Learning (MCL205) `46:17`   - [ ] 16. Continuous Delivery with AWS CodePipeline and Amazon SageMaker `25:24`   - [ ] 17. AWS re:Invent 2017: Best Practices for Distributed Machine Learning and Predictive A (ABD403) `1:16:16`   - [ ] 18. AWS re:Invent 2017: GPS: Enhancing Customer Security Using AI/ML on AWS (GPSTEC311) `50:21`   - [ ] 19. How to Wrangle Data for Machine Learning on AWS `59:24`   - [ ] 20. Extract Data from Images and Videos with Amazon Rekognition (Level 300) `26:52`   - [ ] 21. Exploring the Business Use Cases for Amazon Machine Learning - 2017 AWS Online Tech Talks `30:35`   - [ ] 22. AWS re:Invent 2017: Orchestrating Machine Learning Training for Netflix Recommendation (MCL317) `54:21`   - [ ] 23. AWS re:Invent 2017: Reinforcement Learning - The Ultimate AI (ARC320) `1:00:00`   - [ ] 24. Amazon Machine Learning: Empowering Developers to Build Smart Applications `55:09`   - [ ] 25. Amazon SageMaker's Built-in Algorithm Webinar Series: DeepAR Forecasting `53:41`   - [ ] 26. Amazon SageMaker's Built-in Algorithm Webinar Series: Linear Learner `58:55`   - [ ] 27. Amazon SageMaker's Built-in Algorithm Webinar Series: Clustering with K Means `58:52`   - [ ] 28. Amazon SageMaker's Built-in Algorithm Webinar Series: Latent Dirichlet Allocation (LDA) `57:25`   - [ ] 29. Amazon SageMaker's Built-in Algorithm Webinar Series: XGBoost `1:01:02`   - [ ] 30. Amazon SageMaker's Built-in Algorithm Webinar Series: ResNet `55:56`   - [ ] 31. Amazon SageMaker-s Built-in Algorithm Webinar Series: Blazing Text `1:14:37`   - [ ] 32. AWS re:Invent 2017: NEW LAUNCH! Introducing Amazon SageMaker (MCL365) `1:02:08`   - [ ] 33. Fully Managed Notebook Instances with Amazon SageMaker - a Deep Dive `16:45`   - [ ] 34. Built-in Machine Learning Algorithms with Amazon SageMaker -  a Deep Dive `15:38` - [ ] [Machine Learning with TensorFlow on Google Cloud Platform Specialization](https://www.coursera.org/specializations/machine-learning-tensorflow-gcp) by Google Cloud Training   - [ ] Course 1: How Google does Machine Learning     - [ ] 1. Introduction to Course     - [ ] 2. What It Means to Be AI First     - [ ] 3. How Google Does ML     - [ ] 4. Inclusive ML     - [ ] 5. Python Notebooks in the Cloud     - [ ] 6. Summary   - [ ] Course 2: Launching into Machine Learning     - [ ] 1. Introduction to Course     - [ ] 2. Improve Data Quality and Exploratory Data Analysis     - [ ] 3. Practical ML     - [ ] 4. Optimization     - [ ] 5. Generalization and Sampling     - [ ] 6. Summary   - [ ] Course 3: Introduction to TensorFlow     - [ ] 1. Introduction to Course     - [ ] 2. Introduction to TensorFlow     - [ ] 3. Design and Build a TensorFlow Input Data Pipeline     - [ ] 4. Training Neural Networks with TensorFlow 2 and the Keras Sequential API     - [ ] 5. Training Neural Networks with TensorFlow 2 and the Keras Functional API     - [ ] 6. Summary   - [ ] Course 4: Feature Engineering     - [ ] 1. Introduction to Course     - [ ] 2. Raw Data to Features     - [ ] 3. Preprocessing and Feature Creation     - [ ] 4. Feature Crosses     - [ ] 5. TensorFlow Transform     - [ ] 6. Summary   - [ ] Course 5: Art and Science of Machine Learning     - [ ] 1. Introduction     - [ ] 2. The Art of ML     - [ ] 3. Hyperparameter Tuning     - [ ] 4. A Pinch of Science     - [ ] 5. The Science of Neural Networks     - [ ] 6. Embeddings     - [ ] 7. Summary;false;https://github.com/djdprogramming/adfa2
pipeline framework language:python;transform;tensorflow/transform;Input pipeline framework;false;https://github.com/tensorflow/transform
pipeline framework language:python;fuel;mila-iqia/fuel;A data pipeline framework for machine learning;false;https://github.com/mila-iqia/fuel
pipeline framework language:python;pipelinewise;transferwise/pipelinewise;Data Pipeline Framework using the singer.io spec;false;https://github.com/transferwise/pipelinewise
pipeline framework language:python;mara-pipelines;mara/mara-pipelines;A lightweight opinionated ETL framework, halfway between plain scripts and Apache Airflow;false;https://github.com/mara/mara-pipelines
pipeline framework language:python;PipelineDP;OpenMined/PipelineDP;PipelineDP is a Python framework for applying differentially private aggregations to large datasets using batch processing systems such as Apache Spark, Apache Beam, and more.;false;https://github.com/OpenMined/PipelineDP
pipeline framework language:python;towhee;towhee-io/towhee;Towhee is a framework that is dedicated to making neural data processing pipelines simple and fast.;false;https://github.com/towhee-io/towhee
pipeline framework language:python;Niffler;Emory-HITI/Niffler;Niffler: A DICOM Framework for Machine Learning and Processing Pipelines.;false;https://github.com/Emory-HITI/Niffler
pipeline framework language:python;vehicle-detection;JunshengFu/vehicle-detection;Created vehicle detection pipeline with two approaches: (1) deep neural networks (YOLO framework) and (2) support vector machines ( OpenCV + HOG).;false;https://github.com/JunshengFu/vehicle-detection
pipeline framework language:python;karton;CERT-Polska/karton;Distributed malware processing framework based on Python, Redis and S3.;false;https://github.com/CERT-Polska/karton
pipeline framework language:python;pipen;pwwang/pipen;pipen - A pipeline framework for python;false;https://github.com/pwwang/pipen
pipeline framework language:python;LightAutoML;sberbank-ai-lab/LightAutoML;LAMA - automatic model creation framework;false;https://github.com/sberbank-ai-lab/LightAutoML
pipeline framework language:python;Neuraxle;Neuraxio/Neuraxle;The world's cleanest AutoML library ✨ - Do hyperparameter tuning with the right pipeline abstractions to write clean deep learning production pipelines. Let your pipeline steps have hyperparameter spaces. Design steps in your pipeline like components. Compatible with Scikit-Learn, TensorFlow, and most other libraries, frameworks and MLOps environments.;false;https://github.com/Neuraxio/Neuraxle
pipeline framework language:python;bodywork-core;bodywork-ml/bodywork-core;ML pipeline orchestration and model deployments on Kubernetes.;false;https://github.com/bodywork-ml/bodywork-core
pipeline framework language:python;jina;jina-ai/jina;☁️ Build multimodal AI applications with cloud-native stack;false;https://github.com/jina-ai/jina
pipeline framework language:python;PeekingDuck;aisingapore/PeekingDuck;A modular framework built to simplify Computer Vision inference workloads.;false;https://github.com/aisingapore/PeekingDuck
pipeline framework language:python;mosec;mosecorg/mosec;A high-performance ML model serving framework, offers dynamic batching and CPU/GPU pipelines to fully exploit your compute machine;false;https://github.com/mosecorg/mosec
pipeline framework language:python;cliboa;BrainPad/cliboa;application framework for ETL(ELT) pipeline, process;false;https://github.com/BrainPad/cliboa
pipeline framework language:python;pierogis;pierogis/pierogis;an image and animation processing framework;false;https://github.com/pierogis/pierogis
pipeline framework language:python;dlt-meta;databrickslabs/dlt-meta;This is metadata driven DLT based framework for bronze/silver pipelines;false;https://github.com/databrickslabs/dlt-meta
pipeline framework language:python;serverless-api-cross-account-cicd;awslabs/serverless-api-cross-account-cicd;Build CI/CD Pipeline for Cross Account Deployment of Lambda API Using Serverless Framework;false;https://github.com/awslabs/serverless-api-cross-account-cicd
pipeline framework language:python;pylada-light;pylada/pylada-light;A physics computational framework for python and ipython;false;https://github.com/pylada/pylada-light
pipeline framework language:python;dbnd;databand-ai/dbnd;DBND is an agile pipeline framework that helps data engineering teams track and orchestrate their data processes.;false;https://github.com/databand-ai/dbnd
pipeline framework language:python;multisensor-pipeline;DFKI-Interactive-Machine-Learning/multisensor-pipeline;The core library of the DFKI multisensor pipeline framework.;false;https://github.com/DFKI-Interactive-Machine-Learning/multisensor-pipeline
pipeline framework language:python;bifrost;ledatelescope/bifrost;A stream processing framework for high-throughput applications.;false;https://github.com/ledatelescope/bifrost
pipeline framework language:python;Anomaly-Detection-Pipeline-Kedro;kennethleungty/Anomaly-Detection-Pipeline-Kedro;Anomaly Detection Pipeline with Isolation Forest model and Kedro framework;false;https://github.com/kennethleungty/Anomaly-Detection-Pipeline-Kedro
pipeline framework language:python;Savior;novioleo/Savior;(WIP)The deployment framework aims to provide a simple, lightweight, fast integrated, pipelined deployment framework for algorithm service that ensures reliability, high concurrency and scalability of services.;false;https://github.com/novioleo/Savior
pipeline framework language:python;BioQueue;liyao001/BioQueue;A novel pipeline framework to accelerate bioinformatics analysis;false;https://github.com/liyao001/BioQueue
pipeline framework language:python;wishbone;smetj/wishbone;A Python framework to build composable event pipeline servers with minimal effort.;false;https://github.com/smetj/wishbone
pipeline framework language:python;llm-app;pathwaycom/llm-app;LLM App is a production framework for building and serving AI applications and LLM-enabled real-time data pipelines.;false;https://github.com/pathwaycom/llm-app
pipeline framework language:python;xp;druths/xp;A framework (comand line tool + libraries) for creating flexible compute pipelines;false;https://github.com/druths/xp
pipeline framework language:python;corral;toros-astro/corral;The Powerful Pipeline Framework;false;https://github.com/toros-astro/corral
pipeline framework language:python;MOT-Tracking-by-Detection-Pipeline;Kazuhito00/MOT-Tracking-by-Detection-Pipeline;Tracking-by-Detection形式のMOT(Multi Object Tracking)について、 DetectionとTrackingの処理を分離して寄せ集めたフレームワーク(Tracking-by-Detection method MOT(Multi Object Tracking) is a framework that separates the processing of Detection and Tracking.);false;https://github.com/Kazuhito00/MOT-Tracking-by-Detection-Pipeline
pipeline framework language:python;karton-classifier;CERT-Polska/karton-classifier;File type classifier for the Karton framework.;false;https://github.com/CERT-Polska/karton-classifier
pipeline framework language:python;versatile-data-kit;vmware/versatile-data-kit;One framework to develop, deploy and operate data workflows with Python and SQL.;false;https://github.com/vmware/versatile-data-kit
pipeline framework language:python;haystack;deepset-ai/haystack;:mag: LLM orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data. With advanced retrieval methods, it's best suited for building RAG, question answering, semantic search or conversational agent chatbots.;false;https://github.com/deepset-ai/haystack
pipeline framework language:python;ec2-imagebuilder-poc;Ashex/ec2-imagebuilder-poc;Proof of Concept framework for generating EC2 Image Builder pipelines;false;https://github.com/Ashex/ec2-imagebuilder-poc
pipeline framework language:python;snowflake-on-ecs;slalombuild/snowflake-on-ecs;A compact framework for automating a Snowflake analytics pipeline on Amazon ECS.;false;https://github.com/slalombuild/snowflake-on-ecs
pipeline framework language:python;cmf;HewlettPackard/cmf;CMF library helps to collect and store information associated with ML pipelines.  It tracks the lineages for artifacts and executions of distributed AI pipelines.  It provides API's to record and query the metadata associated with ML pipelines. The framework adopts a data first approach and all artifacts recorded in the framework are versioned and identified by the content hash.;false;https://github.com/HewlettPackard/cmf
pipeline framework language:python;stetl;geopython/stetl;Stetl, Streaming ETL, is a lightweight geospatial processing and ETL framework written in Python.;false;https://github.com/geopython/stetl
pipeline framework language:python;HojiChar;HojiChar/HojiChar;The robust text processing pipeline framework enabling customizable, efficient, and metric-logged text preprocessing.;false;https://github.com/HojiChar/HojiChar
pipeline framework language:python;predict-census-income;ballet/predict-census-income;" A feature engineering pipeline for income prediction using the Ballet framework ";false;https://github.com/ballet/predict-census-income
pipeline framework language:python;Pandora;CNES/Pandora;A stereo matching framework that will help you design your stereo matching pipeline with state of the art performances.;false;https://github.com/CNES/Pandora
pipeline framework language:python;Fast-Api-example;KenMwaura1/Fast-Api-example;Simple asynchronous API implemented with Fast-Api framework utilizing Postgres as a Database and SqlAlchemy as ORM . GitHub Actions as CI/CD Pipeline;false;https://github.com/KenMwaura1/Fast-Api-example
pipeline framework language:python;pipeless;pipeless-ai/pipeless;An open-source computer vision framework to build and deploy apps in minutes without worrying about multimedia pipelines;false;https://github.com/pipeless-ai/pipeless
pipeline framework language:python;ChessAnalysisPipeline;CHESSComputing/ChessAnalysisPipeline;CHESS pipeline framework;false;https://github.com/CHESSComputing/ChessAnalysisPipeline
pipeline framework language:python;buildflow;launchflow/buildflow;BuildFlow, is an open source framework for building large scale systems using Python. All you need to do is describe where your input is coming from and where your output should be written, and BuildFlow handles the rest. No configuration outside of the code is required.;false;https://github.com/launchflow/buildflow
pipeline framework language:python;verify;lsst/verify;lsst.verify - LSST Science Pipelines Verification Framework;false;https://github.com/lsst/verify
pipeline framework language:python;janis-workshops;PMCC-BioinformaticsCore/janis-workshops;Workshops for the Janis pipeline framework;false;https://github.com/PMCC-BioinformaticsCore/janis-workshops
pipeline framework language:python;SmartPipeline;giacbrd/SmartPipeline;A framework for rapid development of robust data pipelines following a simple design pattern;false;https://github.com/giacbrd/SmartPipeline
pipeline framework language:python;chinstrap;ant4g0nist/chinstrap;A development environment, testing framework, and origination pipeline focused solely on Tezos;false;https://github.com/ant4g0nist/chinstrap
pipeline framework language:python;thepipe;tamasgal/thepipe;A simplistic, general purpose pipeline framework.;false;https://github.com/tamasgal/thepipe
pipeline framework language:python;PipeLayer;greater-than/PipeLayer;A lightweight, event-driven, pipeline framework. ;false;https://github.com/greater-than/PipeLayer
pipeline framework language:python;aws-media-replay-engine;awslabs/aws-media-replay-engine;Media Replay Engine (MRE) is a framework to build automated video clipping and replay (highlight) generation pipelines for live and video-on-demand content.;false;https://github.com/awslabs/aws-media-replay-engine
pipeline framework language:python;predict-house-prices;ballet/predict-house-prices;[Playground] A feature engineering pipeline for house price prediction using the Ballet framework;false;https://github.com/ballet/predict-house-prices
pipeline framework language:python;karton-config-extractor;CERT-Polska/karton-config-extractor;Static configuration extractor for the Karton framework;false;https://github.com/CERT-Polska/karton-config-extractor
pipeline framework language:python;amqpipe;Fatal1ty/amqpipe;Twisted based pipeline framework for AMQP;false;https://github.com/Fatal1ty/amqpipe
pipeline framework language:python;ADLStream;pedrolarben/ADLStream;Asynchronous dual-pipeline deep learning framework for online data stream mining;false;https://github.com/pedrolarben/ADLStream
pipeline framework language:python;sparklanes;ksbg/sparklanes;A lightweight data processing framework for Apache Spark;false;https://github.com/ksbg/sparklanes
pipeline framework language:python;karton-yaramatcher;CERT-Polska/karton-yaramatcher;File and analysis artifacts yara matcher for Karton framework;false;https://github.com/CERT-Polska/karton-yaramatcher
pipeline framework language:python;fluidml;fluidml/fluidml;FluidML is a lightweight framework for developing machine learning pipelines.;false;https://github.com/fluidml/fluidml
pipeline framework language:python;karton-archive-extractor;CERT-Polska/karton-archive-extractor;Extractor of various archive formats for Karton framework;false;https://github.com/CERT-Polska/karton-archive-extractor
pipeline framework language:python;Nifty4Gemini;mrlb05/Nifty4Gemini;A Gemini Data Reduction Pipeline Framework;false;https://github.com/mrlb05/Nifty4Gemini
pipeline framework language:python;Rce-KGQA;albert-jin/Rce-KGQA;A novel pipeline framework for multi-hop complex KGQA task. It aims at Improving Multi-hop Embedded Knowledge Graph Question Answering by Introducing Relational Chain Reasoning;false;https://github.com/albert-jin/Rce-KGQA
pipeline framework language:python;spark_etl;stonezhong/spark_etl;Generic ETL Pipeline Framework for Apache Spark;false;https://github.com/stonezhong/spark_etl
pipeline framework language:python;lpipe;mintel/lpipe;🐑Lambda Pipeline, a framework for writing clear, minimal Python FAAS;false;https://github.com/mintel/lpipe
pipeline framework language:python;phylociraptor;reslp/phylociraptor;rapid phylogenomic tree calculator - A highly customizable framework for reproducible phylogenomic inference;false;https://github.com/reslp/phylociraptor
pipeline framework language:python;visualblocks;google/visualblocks;Visual Blocks for ML is a Google visual programming framework that lets you create ML pipelines in a no-code graph editor. You – and your users – can quickly prototype workflows by connecting drag-and-drop ML components, including models, user inputs, processors, and visualizations.;false;https://github.com/google/visualblocks
pipeline framework language:python;Test-Driven-Development-with-Django-REST-Framework-and-Docker;uwevanopfern/Test-Driven-Development-with-Django-REST-Framework-and-Docker;Test-Driven Development with Django, Django REST Framework, Code Coverage, Code Quality, and Docker;false;https://github.com/uwevanopfern/Test-Driven-Development-with-Django-REST-Framework-and-Docker
pipeline framework language:python;builder;unifyai/builder;Build custom Ivy training pipelines for any deep learning framework with clear, hierarchical and robust user specifications;false;https://github.com/unifyai/builder
pipeline framework language:python;debussy_concert;debussy-labs/debussy_concert;Debussy is an opinionated Data Architecture and Engineering framework, enabling data analysts and engineers to build better platforms and pipelines.;false;https://github.com/debussy-labs/debussy_concert
pipeline framework language:python;pytorch-pipeline;tofunlp/pytorch-pipeline;:dart:Simple ETL Framework for PyTorch;false;https://github.com/tofunlp/pytorch-pipeline
pipeline framework language:python;batchout;ilia-khaustov/batchout;Framework for building data pipelines;false;https://github.com/ilia-khaustov/batchout
pipeline framework language:python;mlops-workload-orchestrator;aws-solutions/mlops-workload-orchestrator;The MLOps Workload Orchestrator solution helps you streamline and enforce architecture best practices for machine learning (ML) model productionization. This solution is an extendable framework that provides a standard interface for managing ML pipelines for AWS ML services and third-party services.;false;https://github.com/aws-solutions/mlops-workload-orchestrator
pipeline framework language:python;aswg-pipeline;PeterEckmann1/aswg-pipeline;Framework and set of tools for analyzing common problems in scientific manuscripts.;false;https://github.com/PeterEckmann1/aswg-pipeline
pipeline framework language:python;artellapipe;ArtellaPipe/artellapipe;Framework to create generic Artella based pipelines;false;https://github.com/ArtellaPipe/artellapipe
pipeline framework language:python;pipeline;yifan/pipeline;A data streaming pipeline framework built on Kafka and Pulsar;false;https://github.com/yifan/pipeline
pipeline framework language:python;bleachermark;miguelmarco/bleachermark;A pipeline benchmarking framework for python;false;https://github.com/miguelmarco/bleachermark
pipeline framework language:python;gnn-classification-pipeline;krzysztoffiok/gnn-classification-pipeline;A framework for experimenting with Graph Neural Networks;false;https://github.com/krzysztoffiok/gnn-classification-pipeline
pipeline framework language:python;synth;BennettDixon/synth;CLI tool for building sets of Docker, Docker Compose, and CI/CD pipeline config files, as well as directory trees and wire-frame files for different frameworks;false;https://github.com/BennettDixon/synth
pipeline framework language:python;cognition-pipeline;geospatial-jeff/cognition-pipeline;Python framework for deploying serverless AWS applications.;false;https://github.com/geospatial-jeff/cognition-pipeline
pipeline framework language:python;VAI-Lab;AaltoPML/VAI-Lab;Modular pipeline for design assistance. PYPi Package: https://pypi.org/project/vai-lab;false;https://github.com/AaltoPML/VAI-Lab
pipeline framework language:python;reip-pipelines;reip-project/reip-pipelines;REIP Software Framework;false;https://github.com/reip-project/reip-pipelines
pipeline framework language:python;pypeliner;allonios/pypeliner;a lightweight python framework for creating processing pipelines.;false;https://github.com/allonios/pypeliner
pipeline framework language:python;gcp-airflow-foundations;badal-io/gcp-airflow-foundations;Opinionated framework based on Airflow 2.0 for building pipelines to ingest data into a BigQuery data warehouse;false;https://github.com/badal-io/gcp-airflow-foundations
pipeline framework language:python;web-scraping-pipeline;ELC/web-scraping-pipeline;This is a demo project to compare two web scrapping frameworks, Playwright and Selenium and using the new Pipelining tool Dagster;false;https://github.com/ELC/web-scraping-pipeline
pipeline framework language:python;dataframe-pipeline;IBM/dataframe-pipeline;Machine-learning pipeline framework for pandas and ONNX;false;https://github.com/IBM/dataframe-pipeline
pipeline framework language:python;SparkPipelineFramework.Testing;icanbwell/SparkPipelineFramework.Testing;Testing framework that can tests SPF library by just providing input files to setup before running the transformer and output files to use for verifying the output;false;https://github.com/icanbwell/SparkPipelineFramework.Testing
pipeline framework language:python;fline;asromahin/fline;Pytorch framework for fast create pipeline;false;https://github.com/asromahin/fline
pipeline framework language:python;SIEVE_benchmark_pipeline;szczurek-lab/SIEVE_benchmark_pipeline;Benchmarking framework for SIEVE.;false;https://github.com/szczurek-lab/SIEVE_benchmark_pipeline
pipeline framework language:python;luisy;boschglobal/luisy;A Python framework to build reproducible, robust, and scalable data pipelines;false;https://github.com/boschglobal/luisy
pipeline framework language:python;reliure;kodexlab/reliure;Minimal framework to manage data processing pipelines;false;https://github.com/kodexlab/reliure
pipeline framework language:python;batchy;hsnlab/batchy;Batch-scheduler framework for controlling execution in a packet-processing pipeline based on strict service-level objectives;false;https://github.com/hsnlab/batchy
pipeline framework language:python;edgepipes;joakimeriksson/edgepipes;A small python framework for edge computing pipeline processing (Edge ML/AI);false;https://github.com/joakimeriksson/edgepipes
pipeline framework language:python;pyplant;gleb-t/pyplant;A framework for out-of-core function pipelines;false;https://github.com/gleb-t/pyplant
pipeline framework language:python;mlpipeline;ahmed-shariff/mlpipeline;A framework to consolidate ML workflows;false;https://github.com/ahmed-shariff/mlpipeline
pipeline framework language:python;raposa;xurxodiz/raposa;Lexicological framework for pipeline text processing.;false;https://github.com/xurxodiz/raposa
pipeline dashboard framework;sci-viz;datajoint/sci-viz;Generic visualization framework for building dashboarding capabilities for DataJoint pipelines.;false;https://github.com/datajoint/sci-viz
pipeline dashboard framework;dendro;dendro-monitoring/dendro;Dendro is an open-source, serverless monitoring framework for small, distributed applications. Stream logs and metrics from your various servers to a central time-series store, then explore using our default dashboards and set up alerting!;false;https://github.com/dendro-monitoring/dendro
pipeline dashboard framework;Spark-Data-Pipeline-and-Dashboards;mohitcpatil/Spark-Data-Pipeline-and-Dashboards;This work repository is motivation to learn more about Spark framework, Scala, Python libraries (PySpark, SQL, Matplotlib), and how data warehousing skills are combined to build an analytic application with charts/graphs on a dashboard. With the futuristic approach, it's worth implementing the application with MLlib, Dataset API with Scala version, generating more interactive dashboards with Tableau, Zeppelin, and Plotly.;false;https://github.com/mohitcpatil/Spark-Data-Pipeline-and-Dashboards
pipeline dashboard framework;canopy;canopy-framework/canopy;Canopy is an open-source real-time monitoring framework, designed for use with the Amazon CloudFront CDN. Stream CloudFront logs in real-time to a ClickHouse data store, view a critical suite of metrics on pre-built Grafana dashboards, and set up alerting.;false;https://github.com/canopy-framework/canopy
